{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "# os.environ['KERAS_BACKEND'] = 'theano'\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "from tensorflow import test\n",
    "test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal, ndimage, misc\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape, BatchNormalization, Dropout, GaussianNoise\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Conv2D, Deconv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.layers import Conv3D, UpSampling3D, MaxPooling3D\n",
    "from keras.layers import merge\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras import backend as K_backend\n",
    "from keras import objectives\n",
    "\n",
    "import models_basic\n",
    "\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "from autoencoder import VAE_MNIST_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10, mnist\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def redim(ary, ndim=4):\n",
    "    if ary.ndim == 4: \n",
    "        w, x, y, z = ary.shape\n",
    "    if ary.ndim == 3:\n",
    "        w, x, y = ary.shape\n",
    "        z = 1\n",
    "    if ary.ndim == 2:\n",
    "        w, x = ary.shape\n",
    "        y, z, = 1, 1\n",
    "    if ndim==4:\n",
    "        return ary.reshape((w, x, y, z))\n",
    "    if ndim==3:\n",
    "        return ary.reshape((w, x, y))\n",
    "    if ndim==2:\n",
    "        return ary.reshape((w, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 32, 32, 1) (10000, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "dataset = 'mnist'\n",
    "if dataset == 'mnist':\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = np.pad(x_train, ((0,0), (2,2), (2,2)), 'edge')\n",
    "    x_test = np.pad(x_test, ((0,0), (2,2), (2,2)), 'edge')\n",
    "    \n",
    "if x_train.ndim == 3:\n",
    "    s = x_train.shape\n",
    "    x_train = x_train.reshape((len(x_train), s[1], s[2], 1))\n",
    "    x_test = x_test.reshape((len(x_test), s[1], s[2], 1))\n",
    "original_img_size = x_train[0].shape\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape((x_train.shape[0],) + original_img_size)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape((x_test.shape[0],) + original_img_size)\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "y_train_oh = np.asarray(np_utils.to_categorical(y_train), 'float32')\n",
    "y_test_oh = np.asarray(np_utils.to_categorical(y_test), 'float32')\n",
    "\n",
    "print(y_train_oh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Autoencoder(models_basic.SaveableModel):\n",
    "    \"\"\"\n",
    "    Base class for all-purpose autoencoder. VAE, CNN-AE, etc will be built off of this.\n",
    "\n",
    "    Input -> Encoder -> Z Latent Vector -> Decoder -> Output\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_shape=(28, 28, 1),\n",
    "                 latent_dim=2,  # Size of the encoded vector\n",
    "                 batch_size=100, # size of minibatch\n",
    "                 compile_decoder=False # create the decoder. Not necessary for every use case\n",
    "                 ):\n",
    "        self.model = None\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.batch_size = batch_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.compile_decoder = compile_decoder\n",
    "        assert K_backend.image_dim_ordering() == 'tf', 'Cannot support Theano ordering! Use TF ordering! #tensorflowmasterrace'\n",
    "\n",
    "        # input image dimensions\n",
    "        self.input_shape = input_shape\n",
    "        # self.data_shape = input_shape[1:] # Shape of a single sample\n",
    "        if len(input_shape) == 4:\n",
    "            self.img_rows, self.img_cols, self.img_stacks, self.img_chns = input_shape\n",
    "        elif len(input_shape) == 3:\n",
    "            self.img_rows, self.img_cols, self.img_chns = input_shape\n",
    "        elif len(input_shape) == 2:\n",
    "            self.img_rows, self.img_cols = input_shape\n",
    "            self.img_chns = 1\n",
    "        elif len(input_shape) == 1:\n",
    "            self.img_rows = input_shape[0]  # todo: test this\n",
    "        else:\n",
    "            raise ValueError(\"Invalid input shape: {}\".format(input_shape))\n",
    "\n",
    "    def rollup_decoder(self, z, z_input, layers_list):\n",
    "        \"\"\"\n",
    "        Takes a list of Keras layers and returns the decoder back-half and the standalone decoder model\n",
    "        :param z: Layer corresponding to the latent space vector\n",
    "        :param z_input: Layer corresponding to the decoder input\n",
    "        :param layers_list: List of layers to roll up\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        ae = AE_Dec()\n",
    "        dc = AE_Dec()\n",
    "        last_ae = z\n",
    "        last_dc = z_input\n",
    "        for i, layer in enumerate(layers_list):\n",
    "            #             if i ==0:\n",
    "            last_ae = layer(last_ae)\n",
    "            if self.compile_decoder:\n",
    "                last_dc = layer(last_dc)\n",
    "        return last_ae, last_dc\n",
    "\n",
    "\n",
    "class AE_Dec(object):\n",
    "    \"\"\"\n",
    "    Dummy object for reasons I can't remember. This may be deprecated.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class VAE(Autoencoder):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_shape=(28, 28, 1),\n",
    "                 latent_dim=2,  # Size of the encoded vector\n",
    "                 batch_size=100,  # size of minibatch\n",
    "                 epsilon_std=1.0, # This is the stddev for our normal-dist sampling of the latent vector\n",
    "                 compile_decoder=False\n",
    "                 ):\n",
    "        super().__init__(input_shape=input_shape, latent_dim=latent_dim, batch_size=batch_size,\n",
    "                         compile_decoder=compile_decoder)\n",
    "        # Necessary to instantiate this as instance variables such that they can be passed to the loss function (internally), since loss functions are\n",
    "        # all of the form lossfn(y_true, y_pred)\n",
    "        self.epsilon_std = epsilon_std\n",
    "        self.z_mean = Dense(latent_dim)\n",
    "        self.z_log_var = Dense(latent_dim)\n",
    "        self.beta_kl = K_backend.variable(value=1.0)\n",
    "        \n",
    "        from keras.callbacks import LambdaCallback\n",
    "        self.wu_cb = LambdaCallback(on_epoch_end=lambda epoch, log: self.warmup(epoch))\n",
    "\n",
    "\n",
    "\n",
    "    def sampling(self, args):\n",
    "        \"\"\"\n",
    "        This is what makes the variational technique happen.\n",
    "        :param args:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Forging our latent vector from the reparameterized mean and std requires some sampling trickery\n",
    "        # that admittedly I do not understand in the slightest at this point in time\n",
    "        z_mean, z_log_var = args\n",
    "        batch_size = self.batch_size\n",
    "        epsilon = K_backend.random_normal(shape=(batch_size, self.latent_dim),\n",
    "                                          mean=0., stddev=self.epsilon_std)\n",
    "        # We return z_mean + epsilon*sigma^2. Not sure why we use log var\n",
    "        # Basically, create a random variable vector from the distribution\n",
    "        # We are learning a distribution (mu, var) which represents the input\n",
    "        return z_mean + K_backend.exp(z_log_var) * epsilon\n",
    "\n",
    "    def vae_loss(self, x, x_prime):\n",
    "        \"\"\"\n",
    "        Custom loss function for VAE. Uses Kullback-Leibler divergence.\n",
    "\n",
    "        Notes from fchollet: binary_crossentropy expects a shape (batch_size, dim) for x and x_prime,\n",
    "        so we MUST flatten these!\n",
    "        :param x:\n",
    "        :param x_prime:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        x = K_backend.flatten(x)\n",
    "        x_prime = K_backend.flatten(x_prime)\n",
    "        shape_coef = np.product(self.input_shape)\n",
    "        xent_loss = shape_coef * objectives.binary_crossentropy(x, x_prime)\n",
    "        kl_loss = - 0.5 * K_backend.mean(\n",
    "            1 + self.z_log_var - K_backend.square(self.z_mean) - K_backend.exp(self.z_log_var), axis=-1)\n",
    "        return xent_loss + kl_loss*self.beta_kl\n",
    "    \n",
    "    def vae_loss_kl(self, x, x_prime):\n",
    "        kl_loss = - 0.5 * K_backend.mean(\n",
    "            1 + self.z_log_var - K_backend.square(self.z_mean) - K_backend.exp(self.z_log_var), axis=-1)\n",
    "        return kl_loss\n",
    "    \n",
    "    def vae_loss_xent(self, x, x_prime):\n",
    "        x = K_backend.flatten(x)\n",
    "        x_prime = K_backend.flatten(x_prime)\n",
    "        shape_coef = np.product(self.input_shape)\n",
    "        xent_loss = shape_coef * objectives.binary_crossentropy(x, x_prime)\n",
    "\n",
    "        return xent_loss\n",
    "    \n",
    "    def warmup(self, epoch):\n",
    "        \"\"\"\n",
    "        Trick is based off of: http://stackoverflow.com/questions/42787181/variationnal-auto-encoder-implementing-warm-up-in-keras\n",
    "        \"\"\"\n",
    "        value = (epoch/10.0) * (epoch <= 10.0) + 1.0 * (epoch > 10.0)\n",
    "        value = 1.0 # bypass for now \n",
    "        print(\"beta:\", value)\n",
    "        beta = K_backend.set_value(self.beta_kl, value)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class DiscVAE(VAE):\n",
    "    \"\"\" Covolutional Discriminative VAE\n",
    "     Discriminative Regularization for Generative Models\n",
    "     \"\"\"\n",
    "    coef_xent = 1.\n",
    "    coef_kl = 1.\n",
    "    coef_disc = 0.1\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_shape=(64, 64, 3),\n",
    "                 latent_dim=256,  # Size of the encoded vector\n",
    "                 n_classes=10,  # number of classes in dataset\n",
    "                 batch_size=100,  # size of minibatch\n",
    "                 n_stacks=3,  # Number of convolayers to stack, this boosts performance of the network dramatically\n",
    "                 intermediate_dim=256,  # Size of the dense layer after convs\n",
    "                 n_filters=64,  # Number of filters in the first layer\n",
    "                 px_conv=3,  # Default convolution window size\n",
    "                 dropout_p=0.1,  # Default dropout rate\n",
    "                 epsilon_std=1.0,  # This is the stddev for our normal-dist sampling of the latent vector\n",
    "                 compile_decoder=True,\n",
    "                 ):\n",
    "\n",
    "        # This is my original crossfire network, and it works. As such, it has apprentice marks all over\n",
    "        # Reconstructing as-is before tinkering\n",
    "        # Based heavily on https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder_deconv.py\n",
    "        # and https://groups.google.com/forum/#!msg/keras-users/iBp3Ngxll3k/_GbY4nqNCQAJ\n",
    "\n",
    "        super().__init__(input_shape=input_shape, latent_dim=latent_dim, batch_size=batch_size, epsilon_std=epsilon_std,\n",
    "                         compile_decoder=compile_decoder)\n",
    "        self.new_model(input_shape=input_shape, latent_dim=latent_dim, n_stacks=n_stacks)\n",
    "        \n",
    "    def rollup_disc(self, z, z_input, layers_list, disc_list):\n",
    "        \"\"\"\n",
    "        Takes a list of Keras layers and returns the decoder back-half and the standalone decoder model\n",
    "        :param z: Layer corresponding to the latent space vector\n",
    "        :param z_input: Layer corresponding to the decoder input\n",
    "        :param layers_list: List of layers to roll up\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        last_ae = z\n",
    "        last_disc = z\n",
    "        last_dc = z_input\n",
    "        for i, layer in enumerate(layers_list):\n",
    "            #             if i ==0:\n",
    "            last_ae = layer(last_ae)\n",
    "            last_disc = layer(last_disc)\n",
    "            if self.compile_decoder:\n",
    "                last_dc = layer(last_dc)\n",
    "        for layer in disc_list:\n",
    "            last_disc = layer(last_disc)\n",
    "        return last_ae, last_dc, last_disc\n",
    "\n",
    "        \n",
    "    def discvae_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param x:\n",
    "        :param y: category\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        x = K_backend.flatten(self.x_in)\n",
    "        x_prime = K_backend.flatten(self.x_prime)\n",
    "        shape_coef = np.product(self.input_shape)\n",
    "        xent_loss = shape_coef * objectives.binary_crossentropy(x, x_prime)\n",
    "        disc_loss = objectives.categorical_crossentropy(y_true, y_pred)\n",
    "        kl_loss = - 0.5 * K_backend.mean(\n",
    "            1 + self.z_log_var - K_backend.square(self.z_mean) - K_backend.exp(self.z_log_var), axis=-1)\n",
    "        return xent_loss + kl_loss + disc_loss\n",
    "\n",
    "\n",
    "    def new_model(self,\n",
    "                  input_shape=(64,64,3),\n",
    "                  latent_dim=512,\n",
    "                  kern=3,\n",
    "                  n_classes=10,\n",
    "                  n_filtersX=32,\n",
    "                  n_stacks=4,\n",
    "                  dropout_p=0.2,\n",
    "                  intermediate_dim=512):\n",
    "        # Convolutional frontend filters as per typical convonets\n",
    "        print(self.input_shape)\n",
    "\n",
    "        core_width = input_shape[0] // 2**n_stacks # this will probably break horribly for shapes not power of 2\n",
    "        n_filtersZ = n_filtersX * 2**(n_stacks-1)\n",
    "\n",
    "        x_in = Input(self.input_shape, name='main_input')\n",
    "        self.x_in = x_in\n",
    "        stack = x_in # bit of a hack so we can use a for loop here\n",
    "        \n",
    "        enc_list = [BatchNormalization()] # stack of encoder layers\n",
    "        disc_list = [BatchNormalization()] # stack of discriminator layers\n",
    "        res_list = [] # stack for linking residual layers\n",
    "        \n",
    "        \n",
    "        # pre-stack, don't downsize\n",
    "#         enc_list.append(Conv2D(n_filtersX, (kern, kern), padding='same', activation='relu', name='conv_{}a'.format('p')))\n",
    "#         enc_list.append(BatchNormalization())\n",
    "#         enc_list.append(Activation('relu'))\n",
    "#         enc_list.append(Dropout(dropout_p))\n",
    "#         enc_list.append(Conv2D(n_filtersX, (kern, kern), padding='same', activation='relu', name='conv_{}b'.format('p')))\n",
    "#         enc_list.append(BatchNormalization())\n",
    "#         enc_list.append(Activation('relu'))\n",
    "#         enc_list.append(Conv2D(n_filtersX, (kern, kern), padding='same', activation='relu', name='conv_{}c'.format('p')))\n",
    "\n",
    "\n",
    "        # -> CONV/FC -> BatchNorm -> ReLu(or other activation) -> Dropout -> CONV/FC ->\n",
    "        for i in range(n_stacks):\n",
    "            enc_list.append(Conv2D(n_filtersX * 2 ** i, (kern, kern), padding='same', activation='relu', name='conv_{}a'.format(i)))\n",
    "            enc_list.append(BatchNormalization())\n",
    "            enc_list.append(Activation('relu'))\n",
    "            enc_list.append(Dropout(dropout_p))\n",
    "            enc_list.append(Conv2D(n_filtersX * 2 ** i, (kern, kern), padding='same', activation='relu', name='conv_{}b'.format(i)))\n",
    "            enc_list.append(BatchNormalization())\n",
    "            enc_list.append(Activation('relu'))\n",
    "#             enc_list.append(Conv2D(n_filtersX * 2 ** i, (1, 1), padding='same', activation='relu', name='conv_{}c'.format(i))) # \"Fully Connected Conv\"\n",
    "\n",
    "#             enc_list.append(Dropout(dropout_p)) # dropout of 0.2 here causes network to NaN-out. Weird. \n",
    "            enc_list.append(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "            disc_list.append(Conv2D(n_filtersX * 2 ** i, (kern, kern), padding='same', activation='relu', name='conv_D{}a'.format(i)))\n",
    "            disc_list.append(BatchNormalization())\n",
    "            disc_list.append(Activation('relu'))\n",
    "            disc_list.append(Dropout(dropout_p))\n",
    "            disc_list.append(Conv2D(n_filtersX * 2 ** i, (kern, kern), padding='same', activation='relu', name='conv_D{}b'.format(i)))\n",
    "            disc_list.append(BatchNormalization())\n",
    "            disc_list.append(Activation('relu'))\n",
    "#             disc_list.append(Dropout(dropout_p)) # dropout of 0.2 here causes network to NaN-out. Weird. \n",
    "            disc_list.append(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "\n",
    "        enc_tensors = [x_in]\n",
    "        res_tensors = []\n",
    "        for layer in enc_list:\n",
    "#             stack = layer(stack) # ROLL OUT! connect up all the primary layers\n",
    "            enc_tensors.append(layer(enc_tensors[-1]))\n",
    "        stack = enc_tensors[-1]\n",
    "        \n",
    "        for i in range(len(enc_list)):\n",
    "            print('{: >2}'.format(i), enc_list[i].output_shape, enc_list[i])\n",
    "            \n",
    "            \n",
    "        # Create some deep residual connections. \n",
    "        for i in range(5, len(enc_list), 8):\n",
    "            conv_layer = Conv2D(32, (kern, kern), strides=(2,2), padding='same', activation='relu')(enc_tensors[i])\n",
    "            res_tensors.append(Flatten()(conv_layer))\n",
    "            print(i, enc_list[i])\n",
    "#         res_flattened = [Flatten()(layer) for layer in res_list]\n",
    "#         res_flattened = Dense(10)(res_list[0])\n",
    "        \n",
    "\n",
    "\n",
    "        # Densely connected layer after the filters\n",
    "        flat = Flatten()(stack)\n",
    "        \n",
    "        combined = merge([flat,] + res_tensors, mode='concat')\n",
    "        hidden_1 = Dense(intermediate_dim, activation='relu', name='intermezzo')(flat)\n",
    "\n",
    "        # This is the Variational Autoencoder reparameterization trick\n",
    "        z_mean = Dense(latent_dim)(hidden_1)\n",
    "        z_log_var = Dense(latent_dim)(hidden_1)\n",
    "\n",
    "        # Make these instance vars so X-Ent can use them. Probably a better way out there\n",
    "        self.z_mean = z_mean\n",
    "        self.z_log_var = z_log_var\n",
    "\n",
    "#         # Part 2 of the reparam trick is sample from the mean-vec and std-vec (log_var). To do this, we utilize a\n",
    "#         # custom layer via Lambda class to combine the mean and log_var outputs and a custom sampling function\n",
    "#         # 'z' is our latent vector\n",
    "        z = Lambda(self.sampling, output_shape=(latent_dim,), name='latent_z')([z_mean, z_log_var])\n",
    "#         print(z.shape)\n",
    "\n",
    "#         # This marks the end of the encoding portion of the VAE\n",
    "\n",
    "#         # On to Decoder. we instantiate these layers separately so as to reuse them later\n",
    "#         # e.g. for feeding in latent-space vectors, or (presumably) inspecting output\n",
    "\n",
    "        output_shape = (None, core_width, core_width, n_filtersZ)\n",
    "\n",
    "        decoder_hidden = Dense(intermediate_dim, activation='relu')\n",
    "        decoder_upsample = Dense(n_filtersZ * core_width**2, activation='relu')\n",
    "\n",
    "\n",
    "        decoder_reshape = Reshape(output_shape[1:])\n",
    "\n",
    "        dec_list = [decoder_hidden, decoder_upsample, decoder_reshape]\n",
    "\n",
    "        for i in range(n_stacks-1):\n",
    "            dec_list.append(UpSampling2D((2,2)))\n",
    "            dec_list.append(Deconv2D(n_filtersX * 2 ** (n_stacks-i-1), (kern, kern), padding='same', activation='relu', name='deconv_{}a'.format(i)))\n",
    "            dec_list.append(BatchNormalization())\n",
    "            dec_list.append(Activation('relu'))\n",
    "#             dec_list.append(Dropout(dropout_p))\n",
    "            dec_list.append(Deconv2D(n_filtersX * 2 ** (n_stacks-i-1), (kern, kern), padding='same', activation='relu', name='deconv_{}b'.format(i)))\n",
    "            dec_list.append(BatchNormalization())\n",
    "            dec_list.append(Activation('relu'))\n",
    "#             dec_list.append(Dropout(dropout_p))\n",
    "            \n",
    "        dec_list.append(UpSampling2D((2,2)))\n",
    "        dec_list.append(Conv2D(self.input_shape[2], (kern, kern), padding='same', activation='sigmoid', name='deconv_fin')) # trick to drop down to N channels from filters\n",
    "\n",
    "        decoder_input = Input(shape=(latent_dim,))\n",
    "\n",
    "        # todo: better naming convention\n",
    "        ae, dc = self.rollup_decoder(z, decoder_input, dec_list)\n",
    "        \n",
    "#         ae, dc, disc = self.rollup_disc(z, decoder_input, dec_list, disc_list)\n",
    "#         disc = Flatten()(disc)\n",
    "#         disc = Dense(n_classes, activation='sigmoid')(disc) # classer\n",
    "#         print(type(ae), type(disc))\n",
    "#         print(disc)\n",
    "#         self.model_disc = Model(x_in, disc)\n",
    "#         self.model_disc.compile(optimizer='rmsprop', loss=self.discvae_loss) # loss=self.discvae_loss\n",
    "\n",
    "        self.x_prime = ae\n",
    "\n",
    "#         # Now we create the actual models. We also compile them automatically, this could be isolated later\n",
    "#         # Primary model - VAE\n",
    "        self.model_ae = Model(x_in, ae)\n",
    "        self.model_ae.compile(optimizer='rmsprop', loss=self.vae_loss)\n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "        self.model = self.model_ae\n",
    "        \n",
    "\n",
    "\n",
    "        # build a model to project inputs on the latent space\n",
    "#         self.encoder = Model(x_in, self.z_mean)\n",
    "#         self.foo = Model(x_in, decoder_reshape)\n",
    "        if 0:# self.compile_decoder:\n",
    "            # reconstruct the digit pictures from latent space\n",
    "            self.decoder = Model(decoder_input, dc)\n",
    "\n",
    "    def fit_crossmodel(self, x_dict, y_dict, batch_size=None, nb_epoch=10, verbose=1, callbacks=[],\n",
    "                       validation_split=0.,\n",
    "                       validation_data=None, shuffle=True, class_weight=None, sample_weight=None):\n",
    "        pass\n",
    "        #     callbacks_history = self.crossmodel.fit(x_dict, y_dict, batch_size, nb_epoch, verbose, callbacks,\n",
    "        #                                             validation_split,\n",
    "        #                                             validation_data, shuffle, class_weight, sample_weight)\n",
    "        #     return callbacks_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 1)\n",
      " 0 (None, 32, 32, 1) <keras.layers.normalization.BatchNormalization object at 0x7f18edc73ac8>\n",
      " 1 (None, 32, 32, 32) <keras.layers.convolutional.Conv2D object at 0x7f18e5540198>\n",
      " 2 (None, 32, 32, 32) <keras.layers.normalization.BatchNormalization object at 0x7f18e5540400>\n",
      " 3 (None, 32, 32, 32) <keras.layers.core.Activation object at 0x7f18e5540550>\n",
      " 4 (None, 32, 32, 32) <keras.layers.core.Dropout object at 0x7f18e5540588>\n",
      " 5 (None, 32, 32, 32) <keras.layers.convolutional.Conv2D object at 0x7f18e5540630>\n",
      " 6 (None, 32, 32, 32) <keras.layers.normalization.BatchNormalization object at 0x7f18e55409e8>\n",
      " 7 (None, 32, 32, 32) <keras.layers.core.Activation object at 0x7f18e5540b38>\n",
      " 8 (None, 16, 16, 32) <keras.layers.pooling.MaxPooling2D object at 0x7f18e55406d8>\n",
      " 9 (None, 16, 16, 64) <keras.layers.convolutional.Conv2D object at 0x7f18e55ad1d0>\n",
      "10 (None, 16, 16, 64) <keras.layers.normalization.BatchNormalization object at 0x7f18e55ad320>\n",
      "11 (None, 16, 16, 64) <keras.layers.core.Activation object at 0x7f18e55ad470>\n",
      "12 (None, 16, 16, 64) <keras.layers.core.Dropout object at 0x7f18e55ad4a8>\n",
      "13 (None, 16, 16, 64) <keras.layers.convolutional.Conv2D object at 0x7f18e55ad518>\n",
      "14 (None, 16, 16, 64) <keras.layers.normalization.BatchNormalization object at 0x7f18e55ad668>\n",
      "15 (None, 16, 16, 64) <keras.layers.core.Activation object at 0x7f18e55ad7b8>\n",
      "16 (None, 8, 8, 64) <keras.layers.pooling.MaxPooling2D object at 0x7f18e55ad7f0>\n",
      "5 <keras.layers.convolutional.Conv2D object at 0x7f18e5540630>\n",
      "13 <keras.layers.convolutional.Conv2D object at 0x7f18e55ad518>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:306: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/keras/legacy/layers.py:456: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 32, 32, 1)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1017 (BatchN (None, 32, 32, 1)     4                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_0a (Conv2D)                 (None, 32, 32, 32)    320                                          \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1019 (BatchN (None, 32, 32, 32)    128                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_891 (Activation)      (None, 32, 32, 32)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_360 (Dropout)            (None, 32, 32, 32)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_0b (Conv2D)                 (None, 32, 32, 32)    9248                                         \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1020 (BatchN (None, 32, 32, 32)    128                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_892 (Activation)      (None, 32, 32, 32)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_353 (MaxPooling2D) (None, 16, 16, 32)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_1a (Conv2D)                 (None, 16, 16, 64)    18496                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1023 (BatchN (None, 16, 16, 64)    256                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_895 (Activation)      (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_362 (Dropout)            (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_1b (Conv2D)                 (None, 16, 16, 64)    36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1024 (BatchN (None, 16, 16, 64)    256                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_896 (Activation)      (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_355 (MaxPooling2D) (None, 8, 8, 64)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_193 (Flatten)            (None, 4096)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "intermezzo (Dense)               (None, 512)           2097664                                      \n",
      "____________________________________________________________________________________________________\n",
      "dense_347 (Dense)                (None, 3)             1539                                         \n",
      "____________________________________________________________________________________________________\n",
      "dense_348 (Dense)                (None, 3)             1539                                         \n",
      "____________________________________________________________________________________________________\n",
      "latent_z (Lambda)                (None, 3)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_349 (Dense)                (None, 512)           2048                                         \n",
      "____________________________________________________________________________________________________\n",
      "dense_350 (Dense)                (None, 4096)          2101248                                      \n",
      "____________________________________________________________________________________________________\n",
      "reshape_51 (Reshape)             (None, 8, 8, 64)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_139 (UpSampling2D) (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "deconv_0a (Conv2DTranspose)      (None, 16, 16, 64)    36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1027 (BatchN (None, 16, 16, 64)    256                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_899 (Activation)      (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "deconv_0b (Conv2DTranspose)      (None, 16, 16, 64)    36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1028 (BatchN (None, 16, 16, 64)    256                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_900 (Activation)      (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_140 (UpSampling2D) (None, 32, 32, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "deconv_fin (Conv2D)              (None, 32, 32, 1)     577                                          \n",
      "====================================================================================================\n",
      "Total params: 4,344,747.0\n",
      "Trainable params: 4,344,105.0\n",
      "Non-trainable params: 642.0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import autoencoder\n",
    "from imp import reload\n",
    "reload(autoencoder)\n",
    "\n",
    "# aeclass = autoencoder.VAE_MNIST_0(batch_size=100, n_stacks=0, compile_decoder=False)\n",
    "aeclass = DiscVAE(input_shape=x_train[0].shape, latent_dim=3, n_stacks=2, n_classes=y_train_oh.shape[1])\n",
    "print(aeclass.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keras.utils.vis_utils.plot_model(aeclass.model, 'mymodel.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a63f2f161c64cfc99d9d1cd1dc7e146"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ed753771ed4e398f9a281287fb2075"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c20b11365b4e1fbdee5c673c8af326"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f18e160e978>"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aeclass.model.fit(x_train, x_train, batch_size=100, nb_epoch=2, verbose=0, callbacks=[TQDMNotebookCallback(), aeclass.wu_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# aeclass.model_ae.fit(x_train, x_train, batch_size=100, nb_epoch=2, verbose=0, callbacks=[TQDMNotebookCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pp = aeclass.model_ae.predict(redim(x_test[:200]), batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.996078 0.919056\n",
      "0.0 3.04531e-09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f18e54b1198>"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f18e12e4c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 113\n",
    "if x_test.shape[-1] == 1:\n",
    "    s = pp.shape\n",
    "    pp = pp.reshape((s[0], s[1], s[2]))\n",
    "    x_test = x_test.reshape((len(x_test), s[1], s[2]))\n",
    "    \n",
    "print(np.amax(x_test[n]), np.amax(pp[n]))\n",
    "print(np.amin(x_test[n]), np.amin(pp[n]))\n",
    "plt.imshow(x_test[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f18e3c95748>"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAR8PmwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f18e23299e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(redim(pp[n], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:34: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (2, 2), activation=\"relu\", padding=\"same\")`\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:37: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (2, 2), strides=(2, 2), activation=\"relu\", padding=\"same\")`\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:40: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\")`\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:43: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\")`\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "random_normal() got an unexpected keyword argument 'std'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-220-084895d9268a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# note that \"output_shape\" isn't necessary with the TensorFlow backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# so you could write `Lambda(sampling)([z_mean, z_log_var])`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_log_var\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# we instantiate these layers separately so as to reuse them later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/ve/keras/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;31m# Actually call the layer, collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/ve/keras/lib/python3.5/site-packages/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'mask'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-220-084895d9268a>\u001b[0m in \u001b[0;36msampling\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mz_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_log_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     epsilon = K.random_normal(shape=(batch_size, latent_dim),\n\u001b[0;32m---> 54\u001b[0;31m                               mean=0., std=epsilon_std)\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mz_mean\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_log_var\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: random_normal() got an unexpected keyword argument 'std'"
     ]
    }
   ],
   "source": [
    "'''This script demonstrates how to build a variational autoencoder\n",
    "with Keras and deconvolution layers.\n",
    "\n",
    "Reference: \"Auto-Encoding Variational Bayes\" https://arxiv.org/abs/1312.6114\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape\n",
    "from keras.layers import Convolution2D, Deconvolution2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols, img_chns = 28, 28, 1\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 64\n",
    "# convolution kernel size\n",
    "nb_conv = 3\n",
    "\n",
    "batch_size = 100\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    original_img_size = (img_chns, img_rows, img_cols)\n",
    "else:\n",
    "    original_img_size = (img_rows, img_cols, img_chns)\n",
    "latent_dim = 2\n",
    "intermediate_dim = 128\n",
    "epsilon_std = 0.01\n",
    "nb_epoch = 2\n",
    "\n",
    "x = Input(batch_shape=(batch_size,) + original_img_size)\n",
    "conv_1 = Convolution2D(img_chns, 2, 2, border_mode='same', activation='relu')(x)\n",
    "conv_2 = Convolution2D(nb_filters, 2, 2,\n",
    "                       border_mode='same', activation='relu',\n",
    "                       subsample=(2, 2))(conv_1)\n",
    "conv_3 = Convolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                       border_mode='same', activation='relu',\n",
    "                       subsample=(1, 1))(conv_2)\n",
    "conv_4 = Convolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                       border_mode='same', activation='relu',\n",
    "                       subsample=(1, 1))(conv_3)\n",
    "flat = Flatten()(conv_4)\n",
    "hidden = Dense(intermediate_dim, activation='relu')(flat)\n",
    "\n",
    "z_mean = Dense(latent_dim)(hidden)\n",
    "z_log_var = Dense(latent_dim)(hidden)\n",
    "\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim),\n",
    "                              mean=0., std=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "# so you could write `Lambda(sampling)([z_mean, z_log_var])`\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# we instantiate these layers separately so as to reuse them later\n",
    "decoder_hid = Dense(intermediate_dim, activation='relu')\n",
    "decoder_upsample = Dense(nb_filters * 14 * 14, activation='relu')\n",
    "\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    output_shape = (batch_size, nb_filters, 14, 14)\n",
    "else:\n",
    "    output_shape = (batch_size, 14, 14, nb_filters)\n",
    "\n",
    "decoder_reshape = Reshape(output_shape[1:])\n",
    "decoder_deconv_1 = Deconvolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                                   output_shape,\n",
    "                                   border_mode='same',\n",
    "                                   subsample=(1, 1),\n",
    "                                   activation='relu')\n",
    "decoder_deconv_2 = Deconvolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                                   output_shape,\n",
    "                                   border_mode='same',\n",
    "                                   subsample=(1, 1),\n",
    "                                   activation='relu')\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    output_shape = (batch_size, nb_filters, 29, 29)\n",
    "else:\n",
    "    output_shape = (batch_size, 29, 29, nb_filters)\n",
    "decoder_deconv_3_upsamp = Deconvolution2D(nb_filters, 2, 2,\n",
    "                                          output_shape,\n",
    "                                          border_mode='valid',\n",
    "                                          subsample=(2, 2),\n",
    "                                          activation='relu')\n",
    "decoder_mean_squash = Convolution2D(img_chns, 2, 2,\n",
    "                                    border_mode='valid',\n",
    "                                    activation='sigmoid')\n",
    "\n",
    "hid_decoded = decoder_hid(z)\n",
    "up_decoded = decoder_upsample(hid_decoded)\n",
    "reshape_decoded = decoder_reshape(up_decoded)\n",
    "deconv_1_decoded = decoder_deconv_1(reshape_decoded)\n",
    "deconv_2_decoded = decoder_deconv_2(deconv_1_decoded)\n",
    "x_decoded_relu = decoder_deconv_3_upsamp(deconv_2_decoded)\n",
    "x_prime_squash = decoder_mean_squash(x_decoded_relu)\n",
    "\n",
    "def vae_loss(x, x_prime):\n",
    "    # NOTE: binary_crossentropy expects a batch_size by dim\n",
    "    # for x and x_prime, so we MUST flatten these!\n",
    "    x = K.flatten(x)\n",
    "    x_prime = K.flatten(x_prime)\n",
    "    xent_loss = img_rows * img_cols * objectives.binary_crossentropy(x, x_prime)\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "vae = Model(x, x_prime_squash)\n",
    "vae.compile(optimizer='rmsprop', loss=vae_loss)\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train the VAE on MNIST digits\n",
    "(x_train, _), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train[:, :28, :28]\n",
    "x_test = x_test[:, :28, :28]\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape((x_train.shape[0],) + original_img_size)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape((x_test.shape[0],) + original_img_size)\n",
    "\n",
    "print('x_train.shape:', x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vae.fit(x1, x1,\n",
    "        shuffle=True,\n",
    "        nb_epoch=nb_epoch,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, x_test), \n",
    "       verbose=0, callbacks=[TQDMNotebookCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# build a model to project inputs on the latent space\n",
    "encoder = Model(x, z_mean)\n",
    "\n",
    "# display a 2D plot of the digit classes in the latent space\n",
    "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# build a digit generator that can sample from the learned distribution\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_hid_decoded = decoder_hid(decoder_input)\n",
    "_up_decoded = decoder_upsample(_hid_decoded)\n",
    "_reshape_decoded = decoder_reshape(_up_decoded)\n",
    "_deconv_1_decoded = decoder_deconv_1(_reshape_decoded)\n",
    "_deconv_2_decoded = decoder_deconv_2(_deconv_1_decoded)\n",
    "_x_decoded_relu = decoder_deconv_3_upsamp(_deconv_2_decoded)\n",
    "_x_prime_squash = decoder_mean_squash(_x_decoded_relu)\n",
    "generator = Model(decoder_input, _x_prime_squash)\n",
    "\n",
    "# display a 2D manifold of the digits\n",
    "n = 15  # figure with 15x15 digits\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n, x_train.shape[3]))\n",
    "# we will sample n points within [-15, 15] standard deviations\n",
    "grid_x = np.linspace(-15, 15, n)\n",
    "grid_y = np.linspace(-15, 15, n)\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        z_sample = np.tile(z_sample, batch_size).reshape(batch_size, 2)\n",
    "        x_decoded = generator.predict(z_sample, batch_size=batch_size)\n",
    "        digit = x_decoded[0].reshape((digit_size, digit_size, -1))\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
