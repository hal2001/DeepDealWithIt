{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "# os.environ['KERAS_BACKEND'] = 'theano'\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "from tensorflow import test\n",
    "test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal, ndimage, misc\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape, BatchNormalization, Dropout, GaussianNoise\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Conv2D, Deconv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.layers import Conv3D, UpSampling3D, MaxPooling3D\n",
    "from keras.layers import merge\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras import backend as K_backend\n",
    "from keras import objectives\n",
    "\n",
    "import models_basic\n",
    "\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "from autoencoder import VAE_MNIST_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10, mnist\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def redim(ary, ndim=4):\n",
    "    if ary.ndim == 4: \n",
    "        w, x, y, z = ary.shape\n",
    "    if ary.ndim == 3:\n",
    "        w, x, y = ary.shape\n",
    "        z = 1\n",
    "    if ary.ndim == 2:\n",
    "        w, x = ary.shape\n",
    "        y, z, = 1, 1\n",
    "    if ndim==4:\n",
    "        return ary.reshape((w, x, y, z))\n",
    "    if ndim==3:\n",
    "        return ary.reshape((w, x, y))\n",
    "    if ndim==2:\n",
    "        return ary.reshape((w, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 32, 32, 1) (10000, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "dataset = 'mnist'\n",
    "if dataset == 'mnist':\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = np.pad(x_train, ((0,0), (2,2), (2,2)), 'edge')\n",
    "    x_test = np.pad(x_test, ((0,0), (2,2), (2,2)), 'edge')\n",
    "    \n",
    "if x_train.ndim == 3:\n",
    "    s = x_train.shape\n",
    "    x_train = x_train.reshape((len(x_train), s[1], s[2], 1))\n",
    "    x_test = x_test.reshape((len(x_test), s[1], s[2], 1))\n",
    "original_img_size = x_train[0].shape\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape((x_train.shape[0],) + original_img_size)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape((x_test.shape[0],) + original_img_size)\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "y_train_oh = np.asarray(np_utils.to_categorical(y_train), 'float32')\n",
    "y_test_oh = np.asarray(np_utils.to_categorical(y_test), 'float32')\n",
    "\n",
    "print(y_train_oh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Autoencoder(models_basic.SaveableModel):\n",
    "    \"\"\"\n",
    "    Base class for all-purpose autoencoder. VAE, CNN-AE, etc will be built off of this.\n",
    "\n",
    "    Input -> Encoder -> Z Latent Vector -> Decoder -> Output\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_shape=(28, 28, 1),\n",
    "                 latent_dim=2,  # Size of the encoded vector\n",
    "                 batch_size=100, # size of minibatch\n",
    "                 compile_decoder=False # create the decoder. Not necessary for every use case\n",
    "                 ):\n",
    "        self.model = None\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.batch_size = batch_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.compile_decoder = compile_decoder\n",
    "        assert K_backend.image_dim_ordering() == 'tf', 'Cannot support Theano ordering! Use TF ordering! #tensorflowmasterrace'\n",
    "\n",
    "        # input image dimensions\n",
    "        self.input_shape = input_shape\n",
    "        # self.data_shape = input_shape[1:] # Shape of a single sample\n",
    "        if len(input_shape) == 4:\n",
    "            self.img_rows, self.img_cols, self.img_stacks, self.img_chns = input_shape\n",
    "        elif len(input_shape) == 3:\n",
    "            self.img_rows, self.img_cols, self.img_chns = input_shape\n",
    "        elif len(input_shape) == 2:\n",
    "            self.img_rows, self.img_cols = input_shape\n",
    "            self.img_chns = 1\n",
    "        elif len(input_shape) == 1:\n",
    "            self.img_rows = input_shape[0]  # todo: test this\n",
    "        else:\n",
    "            raise ValueError(\"Invalid input shape: {}\".format(input_shape))\n",
    "\n",
    "    def rollup_decoder(self, z, z_input, layers_list):\n",
    "        \"\"\"\n",
    "        Takes a list of Keras layers and returns the decoder back-half and the standalone decoder model\n",
    "        :param z: Layer corresponding to the latent space vector\n",
    "        :param z_input: Layer corresponding to the decoder input\n",
    "        :param layers_list: List of layers to roll up\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        ae = AE_Dec()\n",
    "        dc = AE_Dec()\n",
    "        last_ae = z\n",
    "        last_dc = z_input\n",
    "        for i, layer in enumerate(layers_list):\n",
    "            #             if i ==0:\n",
    "            last_ae = layer(last_ae)\n",
    "            if self.compile_decoder:\n",
    "                last_dc = layer(last_dc)\n",
    "        return last_ae, last_dc\n",
    "\n",
    "\n",
    "class AE_Dec(object):\n",
    "    \"\"\"\n",
    "    Dummy object for reasons I can't remember. This may be deprecated.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class VAE(Autoencoder):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_shape=(28, 28, 1),\n",
    "                 latent_dim=2,  # Size of the encoded vector\n",
    "                 batch_size=100,  # size of minibatch\n",
    "                 epsilon_std=1.0, # This is the stddev for our normal-dist sampling of the latent vector\n",
    "                 compile_decoder=False\n",
    "                 ):\n",
    "        super().__init__(input_shape=input_shape, latent_dim=latent_dim, batch_size=batch_size,\n",
    "                         compile_decoder=compile_decoder)\n",
    "        # Necessary to instantiate this as instance variables such that they can be passed to the loss function (internally), since loss functions are\n",
    "        # all of the form lossfn(y_true, y_pred)\n",
    "        self.epsilon_std = epsilon_std\n",
    "        self.z_mean = Dense(latent_dim)\n",
    "        self.z_log_var = Dense(latent_dim)\n",
    "\n",
    "\n",
    "\n",
    "    def sampling(self, args):\n",
    "        \"\"\"\n",
    "        This is what makes the variational technique happen.\n",
    "        :param args:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Forging our latent vector from the reparameterized mean and std requires some sampling trickery\n",
    "        # that admittedly I do not understand in the slightest at this point in time\n",
    "        z_mean, z_log_var = args\n",
    "        batch_size = self.batch_size\n",
    "        epsilon = K_backend.random_normal(shape=(batch_size, self.latent_dim),\n",
    "                                          mean=0., stddev=self.epsilon_std)\n",
    "        # We return z_mean + epsilon*sigma^2. Not sure why we use log var\n",
    "        # Basically, create a random variable vector from the distribution\n",
    "        # We are learning a distribution (mu, var) which represents the input\n",
    "        return z_mean + K_backend.exp(z_log_var) * epsilon\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean):\n",
    "        \"\"\"\n",
    "        Custom loss function for VAE. Uses Kullback-Leibler divergence.\n",
    "\n",
    "        Notes from fchollet: binary_crossentropy expects a shape (batch_size, dim) for x and x_decoded_mean,\n",
    "        so we MUST flatten these!\n",
    "        :param x:\n",
    "        :param x_decoded_mean:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        x = K_backend.flatten(x)\n",
    "        x_decoded_mean = K_backend.flatten(x_decoded_mean)\n",
    "        shape_coef = np.product(self.input_shape)\n",
    "        xent_loss = shape_coef * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "        kl_loss = - 0.5 * K_backend.mean(\n",
    "            1 + self.z_log_var - K_backend.square(self.z_mean) - K_backend.exp(self.z_log_var), axis=-1)\n",
    "        return xent_loss + kl_loss\n",
    "\n",
    "\n",
    "class DiscVAE(VAE):\n",
    "    \"\"\" Covolutional Discriminative VAE\n",
    "     Discriminative Regularization for Generative Models\n",
    "     \"\"\"\n",
    "    coef_xent = 1.\n",
    "    coef_kl = 1.\n",
    "    coef_disc = 0.1\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_shape=(64, 64, 3),\n",
    "                 latent_dim=256,  # Size of the encoded vector\n",
    "                 n_classes=10,  # number of classes in dataset\n",
    "                 batch_size=100,  # size of minibatch\n",
    "                 n_stacks=3,  # Number of convolayers to stack, this boosts performance of the network dramatically\n",
    "                 intermediate_dim=256,  # Size of the dense layer after convs\n",
    "                 n_filters=64,  # Number of filters in the first layer\n",
    "                 px_conv=3,  # Default convolution window size\n",
    "                 dropout_p=0.1,  # Default dropout rate\n",
    "                 epsilon_std=1.0,  # This is the stddev for our normal-dist sampling of the latent vector\n",
    "                 compile_decoder=True,\n",
    "                 ):\n",
    "\n",
    "        # This is my original crossfire network, and it works. As such, it has apprentice marks all over\n",
    "        # Reconstructing as-is before tinkering\n",
    "        # Based heavily on https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder_deconv.py\n",
    "        # and https://groups.google.com/forum/#!msg/keras-users/iBp3Ngxll3k/_GbY4nqNCQAJ\n",
    "\n",
    "        super().__init__(input_shape=input_shape, latent_dim=latent_dim, batch_size=batch_size, epsilon_std=epsilon_std,\n",
    "                         compile_decoder=compile_decoder)\n",
    "        self.new_model(input_shape=input_shape, latent_dim=latent_dim, n_stacks=n_stacks)\n",
    "        \n",
    "    def rollup_disc(self, z, z_input, layers_list, disc_list):\n",
    "        \"\"\"\n",
    "        Takes a list of Keras layers and returns the decoder back-half and the standalone decoder model\n",
    "        :param z: Layer corresponding to the latent space vector\n",
    "        :param z_input: Layer corresponding to the decoder input\n",
    "        :param layers_list: List of layers to roll up\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        last_ae = z\n",
    "        last_disc = z\n",
    "        last_dc = z_input\n",
    "        for i, layer in enumerate(layers_list):\n",
    "            #             if i ==0:\n",
    "            last_ae = layer(last_ae)\n",
    "            last_disc = layer(last_disc)\n",
    "            if self.compile_decoder:\n",
    "                last_dc = layer(last_dc)\n",
    "        for layer in disc_list:\n",
    "            last_disc = layer(last_disc)\n",
    "        return last_ae, last_dc, last_disc\n",
    "\n",
    "        \n",
    "    def discvae_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param x:\n",
    "        :param y: category\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        x = K_backend.flatten(self.x_in)\n",
    "        x_decoded_mean = K_backend.flatten(self.x_prime)\n",
    "        shape_coef = np.product(self.input_shape)\n",
    "        xent_loss = shape_coef * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "        disc_loss = objectives.categorical_crossentropy(y_true, y_pred)\n",
    "        kl_loss = - 0.5 * K_backend.mean(\n",
    "            1 + self.z_log_var - K_backend.square(self.z_mean) - K_backend.exp(self.z_log_var), axis=-1)\n",
    "        return xent_loss + kl_loss + disc_loss\n",
    "\n",
    "\n",
    "    def new_model(self,\n",
    "                  input_shape=(64,64,3),\n",
    "                  latent_dim=512,\n",
    "                  kern=3,\n",
    "                  n_classes=10,\n",
    "                  n_filtersX=32,\n",
    "                  n_stacks=4,\n",
    "                  dropout_p=0.2,\n",
    "                  intermediate_dim=512):\n",
    "        # Convolutional frontend filters as per typical convonets\n",
    "        print(self.input_shape)\n",
    "\n",
    "        core_width = input_shape[0] // 2**n_stacks # this will probably break horribly for shapes not power of 2\n",
    "        n_filtersZ = n_filtersX * 2**(n_stacks-1)\n",
    "\n",
    "        x_in = Input(self.input_shape, name='main_input')\n",
    "        self.x_in = x_in\n",
    "        stack = x_in # bit of a hack so we can use a for loop here\n",
    "        \n",
    "        enc_list = [BatchNormalization()] # stack of encoder layers\n",
    "        disc_list = [BatchNormalization()] # stack of discriminator layers\n",
    "        res_list = [] # stack for linking residual layers\n",
    "\n",
    "        # -> CONV/FC -> BatchNorm -> ReLu(or other activation) -> Dropout -> CONV/FC ->\n",
    "        for i in range(n_stacks):\n",
    "            enc_list.append(Conv2D(n_filtersX * 2 ** i, (kern, kern), padding='same', activation='relu', name='conv_{}a'.format(i)))\n",
    "            enc_list.append(BatchNormalization())\n",
    "            enc_list.append(Activation('relu'))\n",
    "            enc_list.append(Dropout(dropout_p))\n",
    "            enc_list.append(Conv2D(n_filtersX * 2 ** i, (kern, kern), padding='same', activation='relu', name='conv_{}b'.format(i)))\n",
    "            enc_list.append(BatchNormalization())\n",
    "            enc_list.append(Activation('relu'))\n",
    "#             enc_list.append(Dropout(dropout_p)) # dropout of 0.2 here causes network to NaN-out. Weird. \n",
    "            enc_list.append(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "            disc_list.append(Conv2D(n_filtersX * 2 ** i, (kern, kern), padding='same', activation='relu', name='conv_D{}a'.format(i)))\n",
    "            disc_list.append(BatchNormalization())\n",
    "            disc_list.append(Activation('relu'))\n",
    "            disc_list.append(Dropout(dropout_p))\n",
    "            disc_list.append(Conv2D(n_filtersX * 2 ** i, (kern, kern), padding='same', activation='relu', name='conv_D{}b'.format(i)))\n",
    "            disc_list.append(BatchNormalization())\n",
    "            disc_list.append(Activation('relu'))\n",
    "#             disc_list.append(Dropout(dropout_p)) # dropout of 0.2 here causes network to NaN-out. Weird. \n",
    "            disc_list.append(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "\n",
    "        enc_tensors = [x_in]\n",
    "        res_tensors = []\n",
    "        for layer in enc_list:\n",
    "#             stack = layer(stack) # ROLL OUT! connect up all the primary layers\n",
    "            enc_tensors.append(layer(enc_tensors[-1]))\n",
    "        stack = enc_tensors[-1]\n",
    "            \n",
    "        for i in range(6, len(enc_list), 8):\n",
    "            conv_layer = Conv2D(32, (kern, kern), strides=(2,2), padding='same', activation='relu')(enc_tensors[i])\n",
    "            res_tensors.append(Flatten()(conv_layer))\n",
    "            print(i, enc_list[i])\n",
    "#         res_flattened = [Flatten()(layer) for layer in res_list]\n",
    "#         res_flattened = Dense(10)(res_list[0])\n",
    "        \n",
    "\n",
    "\n",
    "        # Densely connected layer after the filters\n",
    "        flat = Flatten()(stack)\n",
    "        \n",
    "        combined = merge([flat,] + res_tensors, mode='concat')\n",
    "        hidden_1 = Dense(intermediate_dim, activation='relu', name='intermezzo')(combined)\n",
    "\n",
    "        # This is the Variational Autoencoder reparameterization trick\n",
    "        z_mean = Dense(latent_dim)(hidden_1)\n",
    "        z_log_var = Dense(latent_dim)(hidden_1)\n",
    "\n",
    "        # Make these instance vars so X-Ent can use them. Probably a better way out there\n",
    "        self.z_mean = z_mean\n",
    "        self.z_log_var = z_log_var\n",
    "\n",
    "#         # Part 2 of the reparam trick is sample from the mean-vec and std-vec (log_var). To do this, we utilize a\n",
    "#         # custom layer via Lambda class to combine the mean and log_var outputs and a custom sampling function\n",
    "#         # 'z' is our latent vector\n",
    "        z = Lambda(self.sampling, output_shape=(latent_dim,), name='latent_z')([z_mean, z_log_var])\n",
    "#         print(z.shape)\n",
    "\n",
    "#         # This marks the end of the encoding portion of the VAE\n",
    "\n",
    "#         # On to Decoder. we instantiate these layers separately so as to reuse them later\n",
    "#         # e.g. for feeding in latent-space vectors, or (presumably) inspecting output\n",
    "\n",
    "        output_shape = (None, core_width, core_width, n_filtersZ)\n",
    "\n",
    "        decoder_hidden = Dense(intermediate_dim, activation='relu')\n",
    "        decoder_upsample = Dense(n_filtersZ * core_width**2, activation='relu')\n",
    "\n",
    "\n",
    "        decoder_reshape = Reshape(output_shape[1:])\n",
    "\n",
    "        dec_list = [decoder_hidden, decoder_upsample, decoder_reshape]\n",
    "\n",
    "        for i in range(n_stacks-1):\n",
    "            dec_list.append(UpSampling2D((2,2)))\n",
    "            dec_list.append(Deconv2D(n_filtersX * 2 ** (n_stacks-i-1), (kern, kern), padding='same', activation='relu', name='deconv_{}a'.format(i)))\n",
    "            dec_list.append(BatchNormalization())\n",
    "            dec_list.append(Activation('relu'))\n",
    "#             dec_list.append(Dropout(dropout_p))\n",
    "            dec_list.append(Deconv2D(n_filtersX * 2 ** (n_stacks-i-1), (kern, kern), padding='same', activation='relu', name='deconv_{}b'.format(i)))\n",
    "            dec_list.append(BatchNormalization())\n",
    "            dec_list.append(Activation('relu'))\n",
    "#             dec_list.append(Dropout(dropout_p))\n",
    "            \n",
    "        dec_list.append(UpSampling2D((2,2)))\n",
    "        dec_list.append(Conv2D(self.input_shape[2], (kern, kern), padding='same', activation='sigmoid', name='deconv_fin')) # trick to drop down to N channels from filters\n",
    "\n",
    "        decoder_input = Input(shape=(latent_dim,))\n",
    "\n",
    "        # todo: better naming convention\n",
    "        ae, dc = self.rollup_decoder(z, decoder_input, dec_list)\n",
    "        \n",
    "#         ae, dc, disc = self.rollup_disc(z, decoder_input, dec_list, disc_list)\n",
    "#         disc = Flatten()(disc)\n",
    "#         disc = Dense(n_classes, activation='sigmoid')(disc) # classer\n",
    "#         print(type(ae), type(disc))\n",
    "#         print(disc)\n",
    "#         self.model_disc = Model(x_in, disc)\n",
    "#         self.model_disc.compile(optimizer='rmsprop', loss=self.discvae_loss) # loss=self.discvae_loss\n",
    "\n",
    "        self.x_prime = ae\n",
    "\n",
    "#         # Now we create the actual models. We also compile them automatically, this could be isolated later\n",
    "#         # Primary model - VAE\n",
    "        self.model_ae = Model(x_in, ae)\n",
    "        self.model_ae.compile(optimizer='rmsprop', loss=self.vae_loss)\n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "        self.model = self.model_ae\n",
    "        \n",
    "\n",
    "\n",
    "        # build a model to project inputs on the latent space\n",
    "#         self.encoder = Model(x_in, self.z_mean)\n",
    "#         self.foo = Model(x_in, decoder_reshape)\n",
    "        if 0:# self.compile_decoder:\n",
    "            # reconstruct the digit pictures from latent space\n",
    "            self.decoder = Model(decoder_input, dc)\n",
    "\n",
    "    def fit_crossmodel(self, x_dict, y_dict, batch_size=None, nb_epoch=10, verbose=1, callbacks=[],\n",
    "                       validation_split=0.,\n",
    "                       validation_data=None, shuffle=True, class_weight=None, sample_weight=None):\n",
    "        pass\n",
    "        #     callbacks_history = self.crossmodel.fit(x_dict, y_dict, batch_size, nb_epoch, verbose, callbacks,\n",
    "        #                                             validation_split,\n",
    "        #                                             validation_data, shuffle, class_weight, sample_weight)\n",
    "        #     return callbacks_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 1)\n",
      "6 <keras.layers.normalization.BatchNormalization object at 0x7f19167eb860>\n",
      "14 <keras.layers.normalization.BatchNormalization object at 0x7f191709b5f8>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:257: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/keras/legacy/layers.py:456: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 32, 32, 1)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_609 (BatchNo (None, 32, 32, 1)     4                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_0a (Conv2D)                 (None, 32, 32, 32)    320                                          \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_611 (BatchNo (None, 32, 32, 32)    128                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_551 (Activation)      (None, 32, 32, 32)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_221 (Dropout)            (None, 32, 32, 32)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_0b (Conv2D)                 (None, 32, 32, 32)    9248                                         \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_612 (BatchNo (None, 32, 32, 32)    128                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_552 (Activation)      (None, 32, 32, 32)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_221 (MaxPooling2D) (None, 16, 16, 32)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_1a (Conv2D)                 (None, 16, 16, 64)    18496                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_615 (BatchNo (None, 16, 16, 64)    256                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_555 (Activation)      (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_223 (Dropout)            (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_1b (Conv2D)                 (None, 16, 16, 64)    36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_616 (BatchNo (None, 16, 16, 64)    256                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_556 (Activation)      (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_223 (MaxPooling2D) (None, 8, 8, 64)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)               (None, 16, 16, 32)    9248                                         \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)               (None, 8, 8, 32)      18464                                        \n",
      "____________________________________________________________________________________________________\n",
      "flatten_95 (Flatten)             (None, 4096)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_93 (Flatten)             (None, 8192)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_94 (Flatten)             (None, 2048)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "merge_13 (Merge)                 (None, 14336)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "intermezzo (Dense)               (None, 512)           7340544                                      \n",
      "____________________________________________________________________________________________________\n",
      "dense_152 (Dense)                (None, 3)             1539                                         \n",
      "____________________________________________________________________________________________________\n",
      "dense_153 (Dense)                (None, 3)             1539                                         \n",
      "____________________________________________________________________________________________________\n",
      "latent_z (Lambda)                (None, 3)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_154 (Dense)                (None, 512)           2048                                         \n",
      "____________________________________________________________________________________________________\n",
      "dense_155 (Dense)                (None, 4096)          2101248                                      \n",
      "____________________________________________________________________________________________________\n",
      "reshape_20 (Reshape)             (None, 8, 8, 64)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_75 (UpSampling2D)  (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "deconv_0a (Conv2DTranspose)      (None, 16, 16, 64)    36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_619 (BatchNo (None, 16, 16, 64)    256                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_559 (Activation)      (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "deconv_0b (Conv2DTranspose)      (None, 16, 16, 64)    36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_620 (BatchNo (None, 16, 16, 64)    256                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_560 (Activation)      (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_76 (UpSampling2D)  (None, 32, 32, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "deconv_fin (Conv2D)              (None, 32, 32, 1)     577                                          \n",
      "====================================================================================================\n",
      "Total params: 9,615,339.0\n",
      "Trainable params: 9,614,697.0\n",
      "Non-trainable params: 642.0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import autoencoder\n",
    "from imp import reload\n",
    "reload(autoencoder)\n",
    "\n",
    "# aeclass = autoencoder.VAE_MNIST_0(batch_size=100, n_stacks=0, compile_decoder=False)\n",
    "aeclass = DiscVAE(input_shape=x_train[0].shape, latent_dim=3, n_stacks=2, n_classes=y_train_oh.shape[1])\n",
    "print(aeclass.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keras.utils.vis_utils.plot_model(aeclass.model, 'mymodel.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd2e2d6f0434e5f83ce06b20bcd757d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3faa62ca642f4253b674e4c83e073c65"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19cdd140701049c9ae7c229a61b9b3fc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70f85bbf5f342eabe1e2d42e59ae82c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f190e2acc88>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aeclass.model.fit(x_train, x_train, batch_size=100, nb_epoch=3, verbose=0, callbacks=[TQDMNotebookCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# aeclass.model_ae.fit(x_train, x_train, batch_size=100, nb_epoch=2, verbose=0, callbacks=[TQDMNotebookCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pp = aeclass.model_ae.predict(redim(x_test[:200]), batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.996078 0.813921\n",
      "0.0 0.000658509\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f190dea92b0>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f190dede6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 113\n",
    "if x_test.shape[-1] == 1:\n",
    "    s = pp.shape\n",
    "    pp = pp.reshape((s[0], s[1], s[2]))\n",
    "    x_test = x_test.reshape((len(x_test), s[1], s[2]))\n",
    "    \n",
    "print(np.amax(x_test[n]), np.amax(pp[n]))\n",
    "print(np.amin(x_test[n]), np.amin(pp[n]))\n",
    "plt.imshow(x_test[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f190de0a518>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkShyfiESRc4vRKLI+YVIlP8HpALAxc5vUTkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f190ded54a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(redim(pp[n], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''This script demonstrates how to build a variational autoencoder\n",
    "with Keras and deconvolution layers.\n",
    "\n",
    "Reference: \"Auto-Encoding Variational Bayes\" https://arxiv.org/abs/1312.6114\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape\n",
    "from keras.layers import Convolution2D, Deconvolution2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols, img_chns = 28, 28, 1\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 64\n",
    "# convolution kernel size\n",
    "nb_conv = 3\n",
    "\n",
    "batch_size = 100\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    original_img_size = (img_chns, img_rows, img_cols)\n",
    "else:\n",
    "    original_img_size = (img_rows, img_cols, img_chns)\n",
    "latent_dim = 2\n",
    "intermediate_dim = 128\n",
    "epsilon_std = 0.01\n",
    "nb_epoch = 2\n",
    "\n",
    "x = Input(batch_shape=(batch_size,) + original_img_size)\n",
    "conv_1 = Convolution2D(img_chns, 2, 2, border_mode='same', activation='relu')(x)\n",
    "conv_2 = Convolution2D(nb_filters, 2, 2,\n",
    "                       border_mode='same', activation='relu',\n",
    "                       subsample=(2, 2))(conv_1)\n",
    "conv_3 = Convolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                       border_mode='same', activation='relu',\n",
    "                       subsample=(1, 1))(conv_2)\n",
    "conv_4 = Convolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                       border_mode='same', activation='relu',\n",
    "                       subsample=(1, 1))(conv_3)\n",
    "flat = Flatten()(conv_4)\n",
    "hidden = Dense(intermediate_dim, activation='relu')(flat)\n",
    "\n",
    "z_mean = Dense(latent_dim)(hidden)\n",
    "z_log_var = Dense(latent_dim)(hidden)\n",
    "\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim),\n",
    "                              mean=0., std=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "# so you could write `Lambda(sampling)([z_mean, z_log_var])`\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# we instantiate these layers separately so as to reuse them later\n",
    "decoder_hid = Dense(intermediate_dim, activation='relu')\n",
    "decoder_upsample = Dense(nb_filters * 14 * 14, activation='relu')\n",
    "\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    output_shape = (batch_size, nb_filters, 14, 14)\n",
    "else:\n",
    "    output_shape = (batch_size, 14, 14, nb_filters)\n",
    "\n",
    "decoder_reshape = Reshape(output_shape[1:])\n",
    "decoder_deconv_1 = Deconvolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                                   output_shape,\n",
    "                                   border_mode='same',\n",
    "                                   subsample=(1, 1),\n",
    "                                   activation='relu')\n",
    "decoder_deconv_2 = Deconvolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                                   output_shape,\n",
    "                                   border_mode='same',\n",
    "                                   subsample=(1, 1),\n",
    "                                   activation='relu')\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    output_shape = (batch_size, nb_filters, 29, 29)\n",
    "else:\n",
    "    output_shape = (batch_size, 29, 29, nb_filters)\n",
    "decoder_deconv_3_upsamp = Deconvolution2D(nb_filters, 2, 2,\n",
    "                                          output_shape,\n",
    "                                          border_mode='valid',\n",
    "                                          subsample=(2, 2),\n",
    "                                          activation='relu')\n",
    "decoder_mean_squash = Convolution2D(img_chns, 2, 2,\n",
    "                                    border_mode='valid',\n",
    "                                    activation='sigmoid')\n",
    "\n",
    "hid_decoded = decoder_hid(z)\n",
    "up_decoded = decoder_upsample(hid_decoded)\n",
    "reshape_decoded = decoder_reshape(up_decoded)\n",
    "deconv_1_decoded = decoder_deconv_1(reshape_decoded)\n",
    "deconv_2_decoded = decoder_deconv_2(deconv_1_decoded)\n",
    "x_decoded_relu = decoder_deconv_3_upsamp(deconv_2_decoded)\n",
    "x_decoded_mean_squash = decoder_mean_squash(x_decoded_relu)\n",
    "\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    # NOTE: binary_crossentropy expects a batch_size by dim\n",
    "    # for x and x_decoded_mean, so we MUST flatten these!\n",
    "    x = K.flatten(x)\n",
    "    x_decoded_mean = K.flatten(x_decoded_mean)\n",
    "    xent_loss = img_rows * img_cols * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "vae = Model(x, x_decoded_mean_squash)\n",
    "vae.compile(optimizer='rmsprop', loss=vae_loss)\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train the VAE on MNIST digits\n",
    "(x_train, _), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train[:, :28, :28]\n",
    "x_test = x_test[:, :28, :28]\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape((x_train.shape[0],) + original_img_size)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape((x_test.shape[0],) + original_img_size)\n",
    "\n",
    "print('x_train.shape:', x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vae.fit(x1, x1,\n",
    "        shuffle=True,\n",
    "        nb_epoch=nb_epoch,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, x_test), \n",
    "       verbose=0, callbacks=[TQDMNotebookCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# build a model to project inputs on the latent space\n",
    "encoder = Model(x, z_mean)\n",
    "\n",
    "# display a 2D plot of the digit classes in the latent space\n",
    "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# build a digit generator that can sample from the learned distribution\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_hid_decoded = decoder_hid(decoder_input)\n",
    "_up_decoded = decoder_upsample(_hid_decoded)\n",
    "_reshape_decoded = decoder_reshape(_up_decoded)\n",
    "_deconv_1_decoded = decoder_deconv_1(_reshape_decoded)\n",
    "_deconv_2_decoded = decoder_deconv_2(_deconv_1_decoded)\n",
    "_x_decoded_relu = decoder_deconv_3_upsamp(_deconv_2_decoded)\n",
    "_x_decoded_mean_squash = decoder_mean_squash(_x_decoded_relu)\n",
    "generator = Model(decoder_input, _x_decoded_mean_squash)\n",
    "\n",
    "# display a 2D manifold of the digits\n",
    "n = 15  # figure with 15x15 digits\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n, x_train.shape[3]))\n",
    "# we will sample n points within [-15, 15] standard deviations\n",
    "grid_x = np.linspace(-15, 15, n)\n",
    "grid_y = np.linspace(-15, 15, n)\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        z_sample = np.tile(z_sample, batch_size).reshape(batch_size, 2)\n",
    "        x_decoded = generator.predict(z_sample, batch_size=batch_size)\n",
    "        digit = x_decoded[0].reshape((digit_size, digit_size, -1))\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
