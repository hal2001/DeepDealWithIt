{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep DealWithIt\n",
    "\n",
    "### (•_•)\n",
    "### ( •_•)>⌐■-■\n",
    "### (⌐■_■)\n",
    "\n",
    "\n",
    "This project is based off of the wonderful post by Gabriel Goh, [Decoding the Thought Vector](http://gabgoh.github.io/ThoughtVectors/). My original idea was to train a Variational Autoencoder with [Discriminative Regularization (DR)](https://arxiv.org/abs/1602.03220) on the [Celeb A](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset along with random images from the internet, then use a classifier to locate \"sunglasses thought vector\", and apply that to input images. See the sliders under [Facial Hair and Accessories](http://gabgoh.github.io/ThoughtVectors/#morph_angle), third row, to see what I mean.\n",
    "\n",
    "Unfortunately, I ran into a lot of issues with propagating gradients through very deep VAEs. [This is a known problem with VAEs due to the stochastic layer](https://arxiv.org/abs/1602.02282), and I'm working on getting a Keras implementation of the ladder architecture, but I reckon it won't be ready for the deadline.\n",
    "\n",
    "My plan is: \n",
    "1. Rough it out on MNIST\n",
    "2. Further develop and refine on CIFAR, and then once I've dialed in the DR functionality...\n",
    "3. Train it on Celeb A. \n",
    "4. ???\n",
    "5. PROFIT!\n",
    "\n",
    "I got as far as 2 and a half. But I think the idea of feature vector manipulation is insanely cool, so I'll probably keep hacking away at this for a while. \n",
    "\n",
    "### Note on warmup\n",
    "** Warmup ** is a [technique to improve VAE training](http://orbit.dtu.dk/files/121765928/1602.02282.pdf). The loss function of VAEs consist of two terms: \n",
    "1. Reconstruction loss (binary crossentropy of the input image and the generated image)\n",
    "2. Divergence loss (Kullback-Leibler divergence (KLD) of the stochastic latent Z variable)\n",
    "(1) ensures that the generated image looks like the input image, and (2) esures that the latent Z variable stays (roughly) normal distribution. Warmup decreases the weight of divergence loss at the start of training. My understanding is that the deep convolutional layers need time to \"burn in\", i.e. settle in on filter weights representing useful edges, shapes, abstractions, etc. Weighing KLD too early interferes with the development of useful filters, so we slowly increase the contribution of KLD loss over the course of training. This technique is pretty cutting edge, and seems to be very fiddly and prone to make the model do weird things when not set perfectly correctly by trial-and-error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "# os.environ['KERAS_BACKEND'] = 'theano'\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "from tensorflow import test\n",
    "test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal, ndimage, misc\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape, BatchNormalization, Dropout, GaussianNoise\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Conv2D, Deconv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.layers import Conv3D, UpSampling3D, MaxPooling3D\n",
    "from keras.layers import merge\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras import backend as K_backend\n",
    "from keras import objectives\n",
    "\n",
    "import models_basic\n",
    "\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "from autoencoder import VAE_MNIST_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10, mnist\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def redim(ary, ndim=4):\n",
    "    if ary.ndim == 4: \n",
    "        w, x, y, z = ary.shape\n",
    "    if ary.ndim == 3:\n",
    "        w, x, y = ary.shape\n",
    "        z = 1\n",
    "    if ary.ndim == 2:\n",
    "        w, x = ary.shape\n",
    "        y, z, = 1, 1\n",
    "    if ndim==4:\n",
    "        return ary.reshape((w, x, y, z))\n",
    "    if ndim==3:\n",
    "        return ary.reshape((w, x, y))\n",
    "    if ndim==2:\n",
    "        return ary.reshape((w, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select a dataset. If we use MNIST, pad it out so it's a power of 2. The network expects a shape (N x M), where N and M are powers of two\n",
    "Also, we need to reshape MNIST since it is single channel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "dataset = 'cifar'\n",
    "if dataset == 'mnist':\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = np.pad(x_train, ((0,0), (2,2), (2,2)), 'edge')\n",
    "    x_test = np.pad(x_test, ((0,0), (2,2), (2,2)), 'edge')\n",
    "if dataset == 'cifar':\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "    \n",
    "if x_train.ndim == 3:\n",
    "    s = x_train.shape\n",
    "    x_train = x_train.reshape((len(x_train), s[1], s[2], 1))\n",
    "    x_test = x_test.reshape((len(x_test), s[1], s[2], 1))\n",
    "original_img_size = x_train[0].shape\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape((x_train.shape[0],) + original_img_size)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape((x_test.shape[0],) + original_img_size)\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the Discriminative Regeneration, we would need to use a loss term which contains categorical information\n",
    "This currently isn't operational, so we don't actually need it at the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 10)\n"
     ]
    }
   ],
   "source": [
    "y_train_oh = np.asarray(np_utils.to_categorical(y_train), 'float32')\n",
    "y_test_oh = np.asarray(np_utils.to_categorical(y_test), 'float32')\n",
    "\n",
    "print(y_train_oh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# OOP, there it is!\n",
    "I really love using subclasses in Python to build up layers of abstraction. Since VAE is just a more sophisticated form of autoencoder, we can subclass our general purpose Autoencoder class, and add the machinery necessary to make it a VAE. And since Discriminative VAE is just a more specialized VAE, we can add the regularization secret sauce to this subclass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Autoencoder(models_basic.SaveableModel):\n",
    "    \"\"\"\n",
    "    Base class for all-purpose autoencoder. VAE, CNN-AE, etc will be built off of this.\n",
    "\n",
    "    Input -> Encoder -> Z Latent Vector -> Decoder -> Output\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_shape=(28, 28, 1),\n",
    "                 latent_dim=2,  # Size of the encoded vector\n",
    "                 batch_size=100, # size of minibatch\n",
    "                 compile_decoder=False # create the decoder. Not necessary for every use case\n",
    "                 ):\n",
    "        self.model = None\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.batch_size = batch_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.compile_decoder = compile_decoder\n",
    "        assert K_backend.image_dim_ordering() == 'tf', 'Cannot support Theano ordering! Use TF ordering! #tensorflowmasterrace'\n",
    "\n",
    "        # input image dimensions\n",
    "        self.input_shape = input_shape\n",
    "        # self.data_shape = input_shape[1:] # Shape of a single sample\n",
    "        if len(input_shape) == 4:\n",
    "            self.img_rows, self.img_cols, self.img_stacks, self.img_chns = input_shape\n",
    "        elif len(input_shape) == 3:\n",
    "            self.img_rows, self.img_cols, self.img_chns = input_shape\n",
    "        elif len(input_shape) == 2:\n",
    "            self.img_rows, self.img_cols = input_shape\n",
    "            self.img_chns = 1\n",
    "        elif len(input_shape) == 1:\n",
    "            self.img_rows = input_shape[0]  # todo: test this\n",
    "        else:\n",
    "            raise ValueError(\"Invalid input shape: {}\".format(input_shape))\n",
    "\n",
    "    def rollup_decoder(self, z, z_input, layers_list):\n",
    "        \"\"\"\n",
    "        Takes a list of Keras layers and returns the decoder back-half and the standalone decoder model\n",
    "        :param z: Layer corresponding to the latent space vector\n",
    "        :param z_input: Layer corresponding to the decoder input\n",
    "        :param layers_list: List of layers to roll up\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        last_ae = z\n",
    "        last_dc = z_input\n",
    "        for i, layer in enumerate(layers_list):\n",
    "            #             if i ==0:\n",
    "            last_ae = layer(last_ae)\n",
    "            if self.compile_decoder:\n",
    "                last_dc = layer(last_dc)\n",
    "        return last_ae, last_dc\n",
    "\n",
    "class VAE(Autoencoder):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_shape=(28, 28, 1),\n",
    "                 latent_dim=2,  # Size of the encoded vector\n",
    "                 batch_size=100,  # size of minibatch\n",
    "                 epsilon_std=1.0, # This is the stddev for our normal-dist sampling of the latent vector\n",
    "                 compile_decoder=False\n",
    "                 ):\n",
    "        super().__init__(input_shape=input_shape, latent_dim=latent_dim, batch_size=batch_size,\n",
    "                         compile_decoder=compile_decoder)\n",
    "        # Necessary to instantiate this as instance variables such that they can be passed to the loss function (internally), since loss functions are\n",
    "        # all of the form lossfn(y_true, y_pred)\n",
    "        self.epsilon_std = epsilon_std\n",
    "        self.z_mean = Dense(latent_dim)\n",
    "        self.z_log_var = Dense(latent_dim)\n",
    "        self.beta_kl = K_backend.variable(value=0.1)\n",
    "        self.warmup_ramp_len = 10\n",
    "        \n",
    "        from keras.callbacks import LambdaCallback\n",
    "        self.wu_cb = LambdaCallback(on_epoch_end=lambda epoch, log: self.warmup(epoch))\n",
    "\n",
    "\n",
    "\n",
    "    def sampling(self, args):\n",
    "        \"\"\"\n",
    "        This is what makes the variational technique happen.\n",
    "        :param args:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Forging our latent vector from the reparameterized mean and std requires some sampling trickery\n",
    "        # that admittedly I do not understand in the slightest at this point in time\n",
    "        z_mean, z_log_var = args\n",
    "        batch_size = self.batch_size\n",
    "        epsilon = K_backend.random_normal(shape=(batch_size, self.latent_dim),\n",
    "                                          mean=0., stddev=self.epsilon_std)\n",
    "        # We return z_mean + epsilon*sigma^2. Not sure why we use log var\n",
    "        # Basically, create a random variable vector from the distribution\n",
    "        # We are learning a distribution (mu, var) which represents the input\n",
    "        return z_mean + K_backend.exp(z_log_var) * epsilon\n",
    "\n",
    "    def vae_loss(self, x, x_prime):\n",
    "        \"\"\"\n",
    "        Custom loss function for VAE. Uses Kullback-Leibler divergence.\n",
    "\n",
    "        Notes from fchollet: binary_crossentropy expects a shape (batch_size, dim) for x and x_prime,\n",
    "        so we MUST flatten these!\n",
    "        :param x:\n",
    "        :param x_prime:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        x = K_backend.flatten(x)\n",
    "        x_prime = K_backend.flatten(x_prime)\n",
    "        shape_coef = np.product(self.input_shape)\n",
    "        xent_loss = shape_coef * objectives.binary_crossentropy(x, x_prime)\n",
    "        kl_loss = - 0.5 * K_backend.mean(\n",
    "            1 + self.z_log_var - K_backend.square(self.z_mean) - K_backend.exp(self.z_log_var), axis=-1)\n",
    "        return xent_loss + kl_loss*self.beta_kl\n",
    "    \n",
    "    def warmup(self, epoch):\n",
    "        \"\"\"\n",
    "        Trick is based off of: http://stackoverflow.com/questions/42787181/variationnal-auto-encoder-implementing-warm-up-in-keras\n",
    "        \"\"\"\n",
    "        ramp = self.warmup_ramp_len\n",
    "        value = (epoch/ramp) * (epoch <= ramp) + 1.0 * (epoch > ramp)\n",
    "#         value = 0.2 # bypass for now \n",
    "        print(\"beta:\", value)\n",
    "        beta = K_backend.set_value(self.beta_kl, value)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class DiscVAE(VAE):\n",
    "    \"\"\" Covolutional Discriminative VAE\n",
    "     Discriminative Regularization for Generative Models\n",
    "     \"\"\"\n",
    "    coef_xent = 1.\n",
    "    coef_kl = 1.\n",
    "    coef_disc = 0.1\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_shape=(64, 64, 3),\n",
    "                 latent_dim=256,  # Size of the encoded vector\n",
    "                 n_classes=10,  # number of classes in dataset\n",
    "                 batch_size=100,  # size of minibatch\n",
    "                 n_stacks=3,  # Number of convolayers to stack, this boosts performance of the network dramatically\n",
    "                 intermediate_dim=256,  # Size of the dense layer after convs\n",
    "                 n_filters=64,  # Number of filters in the first layer\n",
    "                 px_conv=3,  # Default convolution window size\n",
    "                 dropout_p=0.1,  # Default dropout rate\n",
    "                 epsilon_std=1.0,  # This is the stddev for our normal-dist sampling of the latent vector\n",
    "                 compile_decoder=True,\n",
    "                 ):\n",
    "\n",
    "        # This is my original crossfire network, and it works. As such, it has apprentice marks all over\n",
    "        # Reconstructing as-is before tinkering\n",
    "        # Based heavily on https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder_deconv.py\n",
    "        # and https://groups.google.com/forum/#!msg/keras-users/iBp3Ngxll3k/_GbY4nqNCQAJ\n",
    "\n",
    "        super().__init__(input_shape=input_shape, latent_dim=latent_dim, batch_size=batch_size, epsilon_std=epsilon_std,\n",
    "                         compile_decoder=compile_decoder)\n",
    "        self.new_model(input_shape=input_shape, latent_dim=latent_dim, n_stacks=n_stacks)\n",
    "        \n",
    "    def rollup_disc(self, z, z_input, layers_list, disc_list):\n",
    "        \"\"\"\n",
    "        Takes a list of Keras layers and returns the decoder back-half and the standalone decoder model\n",
    "        :param z: Layer corresponding to the latent space vector\n",
    "        :param z_input: Layer corresponding to the decoder input\n",
    "        :param layers_list: List of layers to roll up\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        last_ae = z\n",
    "        last_disc = z\n",
    "        last_dc = z_input\n",
    "        for i, layer in enumerate(layers_list):\n",
    "            #             if i ==0:\n",
    "            last_ae = layer(last_ae)\n",
    "            last_disc = layer(last_disc)\n",
    "            if self.compile_decoder:\n",
    "                last_dc = layer(last_dc)\n",
    "        for layer in disc_list:\n",
    "            last_disc = layer(last_disc)\n",
    "        return last_ae, last_dc, last_disc\n",
    "\n",
    "        \n",
    "    def discvae_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param x:\n",
    "        :param y: category\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        x = K_backend.flatten(self.x_in)\n",
    "        x_prime = K_backend.flatten(self.x_prime)\n",
    "        shape_coef = np.product(self.input_shape)\n",
    "        xent_loss = shape_coef * objectives.binary_crossentropy(x, x_prime)\n",
    "        disc_loss = objectives.categorical_crossentropy(y_true, y_pred)\n",
    "        kl_loss = - 0.5 * K_backend.mean(\n",
    "            1 + self.z_log_var - K_backend.square(self.z_mean) - K_backend.exp(self.z_log_var), axis=-1)\n",
    "        return xent_loss + kl_loss + disc_loss\n",
    "\n",
    "\n",
    "    def new_model(self,\n",
    "                  input_shape=(64,64,3),\n",
    "                  latent_dim=512,\n",
    "                  kern=3,\n",
    "                  n_classes=10,\n",
    "                  n_filtersX=32,\n",
    "                  n_stacks=4,\n",
    "                  dropout_p=0.2,\n",
    "                  intermediate_dim=512):\n",
    "        # Convolutional frontend filters as per typical convonets\n",
    "        print(self.input_shape)\n",
    "\n",
    "        core_width = input_shape[0] // 2**n_stacks # this will probably break horribly for shapes not power of 2\n",
    "        n_filtersZ = n_filtersX * 2**(n_stacks-1)\n",
    "\n",
    "        x_in = Input(self.input_shape, name='main_input')\n",
    "        self.x_in = x_in\n",
    "        stack = x_in # bit of a hack so we can use a for loop here\n",
    "        \n",
    "        enc_list = [BatchNormalization()] # stack of encoder layers\n",
    "        disc_list = [BatchNormalization()] # stack of discriminator layers\n",
    "        res_list = [] # stack for linking residual layers\n",
    "        \n",
    "        \n",
    "        # pre-stack, don't downsize\n",
    "#         enc_list.append(Conv2D(n_filtersX, (kern, kern), padding='same', activation='relu', name='conv_{}a'.format('p')))\n",
    "#         enc_list.append(BatchNormalization())\n",
    "#         enc_list.append(Activation('relu'))\n",
    "#         enc_list.append(Dropout(dropout_p))\n",
    "#         enc_list.append(Conv2D(n_filtersX, (kern, kern), padding='same', activation='relu', name='conv_{}b'.format('p')))\n",
    "#         enc_list.append(BatchNormalization())\n",
    "#         enc_list.append(Activation('relu'))\n",
    "#         enc_list.append(Conv2D(n_filtersX, (kern, kern), padding='same', activation='relu', name='conv_{}c'.format('p')))\n",
    "\n",
    "\n",
    "        # -> CONV/FC -> BatchNorm -> ReLu(or other activation) -> Dropout -> CONV/FC ->\n",
    "        for i in range(n_stacks):\n",
    "            enc_list.append(Conv2D(n_filtersX * 2 ** i, (kern, kern), padding='same', activation='relu', name='conv_{}a'.format(i)))\n",
    "            enc_list.append(BatchNormalization())\n",
    "            enc_list.append(Activation('relu'))\n",
    "            enc_list.append(Dropout(dropout_p))\n",
    "            enc_list.append(Conv2D(n_filtersX * 2 ** i, (kern, kern), padding='same', activation='relu', name='conv_{}b'.format(i)))\n",
    "            enc_list.append(BatchNormalization())\n",
    "            enc_list.append(Activation('relu'))\n",
    "#             enc_list.append(Conv2D(n_filtersX * 2 ** i, (1, 1), padding='same', activation='relu', name='conv_{}c'.format(i))) # \"Fully Connected Conv\"\n",
    "\n",
    "#             enc_list.append(Dropout(dropout_p)) # dropout of 0.2 here causes network to NaN-out. Weird. \n",
    "            enc_list.append(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "            disc_list.append(Conv2D(n_filtersX * 2 ** i, (kern, kern), padding='same', activation='relu', name='conv_D{}a'.format(i)))\n",
    "            disc_list.append(BatchNormalization())\n",
    "            disc_list.append(Activation('relu'))\n",
    "            disc_list.append(Dropout(dropout_p))\n",
    "            disc_list.append(Conv2D(n_filtersX * 2 ** i, (kern, kern), padding='same', activation='relu', name='conv_D{}b'.format(i)))\n",
    "            disc_list.append(BatchNormalization())\n",
    "            disc_list.append(Activation('relu'))\n",
    "#             disc_list.append(Dropout(dropout_p)) # dropout of 0.2 here causes network to NaN-out. Weird. \n",
    "            disc_list.append(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "\n",
    "        enc_tensors = [x_in]\n",
    "        res_tensors = []\n",
    "        for layer in enc_list:\n",
    "#             stack = layer(stack) # ROLL OUT! connect up all the primary layers\n",
    "            enc_tensors.append(layer(enc_tensors[-1]))\n",
    "        stack = enc_tensors[-1]\n",
    "        \n",
    "        for i in range(len(enc_list)):\n",
    "            print('{: >2}'.format(i), enc_list[i].output_shape, enc_list[i])\n",
    "            \n",
    "            \n",
    "        # Create some deep residual connections. \n",
    "        for i in range(5, len(enc_list), 8):\n",
    "            conv_layer = Conv2D(32, (kern, kern), strides=(2,2), padding='same', activation='relu')(enc_tensors[i])\n",
    "            res_tensors.append(Flatten()(conv_layer))\n",
    "            print(i, enc_list[i])\n",
    "#         res_flattened = [Flatten()(layer) for layer in res_list]\n",
    "#         res_flattened = Dense(10)(res_list[0])\n",
    "        \n",
    "\n",
    "\n",
    "        # Densely connected layer after the filters\n",
    "        flat = Flatten()(stack)\n",
    "        \n",
    "        combined = merge([flat,] + res_tensors, mode='concat')\n",
    "        hidden_1 = Dense(intermediate_dim, activation='relu', name='intermezzo')(flat)\n",
    "\n",
    "        # This is the Variational Autoencoder reparameterization trick\n",
    "        z_mean = Dense(latent_dim)(hidden_1)\n",
    "        z_log_var = Dense(latent_dim)(hidden_1)\n",
    "\n",
    "        # Make these instance vars so X-Ent can use them. Probably a better way out there\n",
    "        self.z_mean = z_mean\n",
    "        self.z_log_var = z_log_var\n",
    "\n",
    "#         # Part 2 of the reparam trick is sample from the mean-vec and std-vec (log_var). To do this, we utilize a\n",
    "#         # custom layer via Lambda class to combine the mean and log_var outputs and a custom sampling function\n",
    "#         # 'z' is our latent vector\n",
    "        z = Lambda(self.sampling, output_shape=(latent_dim,), name='latent_z')([z_mean, z_log_var])\n",
    "#         print(z.shape)\n",
    "\n",
    "#         # This marks the end of the encoding portion of the VAE\n",
    "\n",
    "#         # On to Decoder. we instantiate these layers separately so as to reuse them later\n",
    "#         # e.g. for feeding in latent-space vectors, or (presumably) inspecting output\n",
    "\n",
    "        output_shape = (None, core_width, core_width, n_filtersZ)\n",
    "\n",
    "        decoder_hidden = Dense(intermediate_dim, activation='relu')\n",
    "        decoder_upsample = Dense(n_filtersZ * core_width**2, activation='relu')\n",
    "\n",
    "\n",
    "        decoder_reshape = Reshape(output_shape[1:])\n",
    "\n",
    "        dec_list = [decoder_hidden, decoder_upsample, decoder_reshape]\n",
    "\n",
    "        for i in range(n_stacks-1):\n",
    "            dec_list.append(UpSampling2D((2,2)))\n",
    "            dec_list.append(Deconv2D(n_filtersX * 2 ** (n_stacks-i-1), (kern, kern), padding='same', activation='relu', name='deconv_{}a'.format(i)))\n",
    "            dec_list.append(BatchNormalization())\n",
    "            dec_list.append(Activation('relu'))\n",
    "#             dec_list.append(Dropout(dropout_p))\n",
    "            dec_list.append(Deconv2D(n_filtersX * 2 ** (n_stacks-i-1), (kern, kern), padding='same', activation='relu', name='deconv_{}b'.format(i)))\n",
    "            dec_list.append(BatchNormalization())\n",
    "            dec_list.append(Activation('relu'))\n",
    "#             dec_list.append(Dropout(dropout_p))\n",
    "            \n",
    "        dec_list.append(UpSampling2D((2,2)))\n",
    "        dec_list.append(Conv2D(self.input_shape[2], (kern, kern), padding='same', activation='sigmoid', name='deconv_fin')) # trick to drop down to N channels from filters\n",
    "\n",
    "        decoder_input = Input(shape=(latent_dim,))\n",
    "\n",
    "        # todo: better naming convention\n",
    "        ae, dc = self.rollup_decoder(z, decoder_input, dec_list)\n",
    "        \n",
    "#         ae, dc, disc = self.rollup_disc(z, decoder_input, dec_list, disc_list)\n",
    "#         disc = Flatten()(disc)\n",
    "#         disc = Dense(n_classes, activation='sigmoid')(disc) # classer\n",
    "#         print(type(ae), type(disc))\n",
    "#         print(disc)\n",
    "#         self.model_disc = Model(x_in, disc)\n",
    "#         self.model_disc.compile(optimizer='rmsprop', loss=self.discvae_loss) # loss=self.discvae_loss\n",
    "\n",
    "        self.x_prime = ae\n",
    "\n",
    "#         # Now we create the actual models. We also compile them automatically, this could be isolated later\n",
    "#         # Primary model - VAE\n",
    "        self.model_ae = Model(x_in, ae)\n",
    "        self.model_ae.compile(optimizer='rmsprop', loss=self.vae_loss)\n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "        self.model = self.model_ae\n",
    "        \n",
    "\n",
    "\n",
    "        # build a model to project inputs on the latent space\n",
    "#         self.encoder = Model(x_in, self.z_mean)\n",
    "#         self.foo = Model(x_in, decoder_reshape)\n",
    "        if self.compile_decoder:\n",
    "            # reconstruct the digit pictures from latent space\n",
    "            self.decoder = Model(decoder_input, dc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n",
      " 0 (None, 32, 32, 3) <keras.layers.normalization.BatchNormalization object at 0x7f19987aa978>\n",
      " 1 (None, 32, 32, 32) <keras.layers.convolutional.Conv2D object at 0x7f19987ae208>\n",
      " 2 (None, 32, 32, 32) <keras.layers.normalization.BatchNormalization object at 0x7f19987ae358>\n",
      " 3 (None, 32, 32, 32) <keras.layers.core.Activation object at 0x7f19987ae4a8>\n",
      " 4 (None, 32, 32, 32) <keras.layers.core.Dropout object at 0x7f19987ae4e0>\n",
      " 5 (None, 32, 32, 32) <keras.layers.convolutional.Conv2D object at 0x7f19987ae550>\n",
      " 6 (None, 32, 32, 32) <keras.layers.normalization.BatchNormalization object at 0x7f19987ae6a0>\n",
      " 7 (None, 32, 32, 32) <keras.layers.core.Activation object at 0x7f19987ae7f0>\n",
      " 8 (None, 16, 16, 32) <keras.layers.pooling.MaxPooling2D object at 0x7f19987ae828>\n",
      " 9 (None, 16, 16, 64) <keras.layers.convolutional.Conv2D object at 0x7f19987aef98>\n",
      "10 (None, 16, 16, 64) <keras.layers.normalization.BatchNormalization object at 0x7f19987b2128>\n",
      "11 (None, 16, 16, 64) <keras.layers.core.Activation object at 0x7f19987b2278>\n",
      "12 (None, 16, 16, 64) <keras.layers.core.Dropout object at 0x7f19987b22b0>\n",
      "13 (None, 16, 16, 64) <keras.layers.convolutional.Conv2D object at 0x7f19987b2320>\n",
      "14 (None, 16, 16, 64) <keras.layers.normalization.BatchNormalization object at 0x7f19987b2470>\n",
      "15 (None, 16, 16, 64) <keras.layers.core.Activation object at 0x7f19987b25c0>\n",
      "16 (None, 8, 8, 64) <keras.layers.pooling.MaxPooling2D object at 0x7f19987b25f8>\n",
      "5 <keras.layers.convolutional.Conv2D object at 0x7f19987ae550>\n",
      "13 <keras.layers.convolutional.Conv2D object at 0x7f19987b2320>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:284: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/keras/legacy/layers.py:456: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 32, 32, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1089 (BatchN (None, 32, 32, 3)     12                                           \n",
      "____________________________________________________________________________________________________\n",
      "conv_0a (Conv2D)                 (None, 32, 32, 32)    896                                          \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1091 (BatchN (None, 32, 32, 32)    128                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_951 (Activation)      (None, 32, 32, 32)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_384 (Dropout)            (None, 32, 32, 32)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_0b (Conv2D)                 (None, 32, 32, 32)    9248                                         \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1092 (BatchN (None, 32, 32, 32)    128                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_952 (Activation)      (None, 32, 32, 32)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_377 (MaxPooling2D) (None, 16, 16, 32)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_1a (Conv2D)                 (None, 16, 16, 64)    18496                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1095 (BatchN (None, 16, 16, 64)    256                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_955 (Activation)      (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_386 (Dropout)            (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_1b (Conv2D)                 (None, 16, 16, 64)    36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1096 (BatchN (None, 16, 16, 64)    256                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_956 (Activation)      (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_379 (MaxPooling2D) (None, 8, 8, 64)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_211 (Flatten)            (None, 4096)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "intermezzo (Dense)               (None, 512)           2097664                                      \n",
      "____________________________________________________________________________________________________\n",
      "dense_383 (Dense)                (None, 3)             1539                                         \n",
      "____________________________________________________________________________________________________\n",
      "dense_384 (Dense)                (None, 3)             1539                                         \n",
      "____________________________________________________________________________________________________\n",
      "latent_z (Lambda)                (None, 3)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_385 (Dense)                (None, 512)           2048                                         \n",
      "____________________________________________________________________________________________________\n",
      "dense_386 (Dense)                (None, 4096)          2101248                                      \n",
      "____________________________________________________________________________________________________\n",
      "reshape_57 (Reshape)             (None, 8, 8, 64)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_151 (UpSampling2D) (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "deconv_0a (Conv2DTranspose)      (None, 16, 16, 64)    36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1099 (BatchN (None, 16, 16, 64)    256                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_959 (Activation)      (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "deconv_0b (Conv2DTranspose)      (None, 16, 16, 64)    36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1100 (BatchN (None, 16, 16, 64)    256                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_960 (Activation)      (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_152 (UpSampling2D) (None, 32, 32, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "deconv_fin (Conv2D)              (None, 32, 32, 3)     1731                                         \n",
      "====================================================================================================\n",
      "Total params: 4,346,485.0\n",
      "Trainable params: 4,345,839.0\n",
      "Non-trainable params: 646.0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import autoencoder\n",
    "from imp import reload\n",
    "reload(autoencoder)\n",
    "\n",
    "# aeclass = autoencoder.VAE_MNIST_0(batch_size=100, n_stacks=0, compile_decoder=False)\n",
    "aeclass = DiscVAE(input_shape=x_train[0].shape, latent_dim=3, n_stacks=2, n_classes=y_train_oh.shape[1])\n",
    "print(aeclass.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keras.utils.vis_utils.plot_model(aeclass.model, 'mymodel.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b05c433174dd47bfad11d95e1cf4b462"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9deaa8ada39f4374ac8f6d6263bfaabe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5feeeac6d340a7814ea467b6f7b43e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa4e8d9dbebb462c84758a748f8b1911"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5ea886bead4affbd74914c6553355a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ddb45939f1413a9a5ad586761d37f3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e256ab2f491b46feb9c281bca5e3b6a5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8669fb6fd9c34fa4a380e0d7ee80e996"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8efd15cb3cd247e183647cad7879c468"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3683ca593cf94df1b6d5c90edc81f49c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde071e913f04f909a953877505a4dfc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f199728cac8>"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aeclass.model.fit(x_train, x_train, batch_size=100, nb_epoch=10, verbose=0, callbacks=[TQDMNotebookCallback(), aeclass.wu_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# aeclass.model_ae.fit(x_train, x_train, batch_size=100, nb_epoch=2, verbose=0, callbacks=[TQDMNotebookCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pp = aeclass.model_ae.predict(redim(x_test[:200]), batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.807843 0.662155\n",
      "0.027451 0.304242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1996eca6d8>"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nioJfiExR8AuRKf8LQb1+g/mS4D4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f18de8092e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 113\n",
    "if x_test.shape[-1] == 1:\n",
    "    s = pp.shape\n",
    "    pp = pp.reshape((s[0], s[1], s[2]))\n",
    "    x_test = x_test.reshape((len(x_test), s[1], s[2]))\n",
    "    \n",
    "print(np.amax(x_test[n]), np.amax(pp[n]))\n",
    "print(np.amin(x_test[n]), np.amin(pp[n]))\n",
    "plt.imshow(x_test[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1996634e80>"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f199666bd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(redim(pp[n], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
