{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "# os.environ['KERAS_BACKEND'] = 'theano'\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "from tensorflow import test\n",
    "test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal, ndimage, misc\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape, BatchNormalization, Dropout, GaussianNoise\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Conv2D, Deconv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.layers import Conv3D, UpSampling3D, MaxPooling3D\n",
    "from keras.layers import merge\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras import backend as K_backend\n",
    "from keras import objectives\n",
    "\n",
    "import models_basic\n",
    "\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "from autoencoder import VAE_MNIST_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10, mnist\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def redim(ary, ndim=4):\n",
    "    if ary.ndim == 4: \n",
    "        w, x, y, z = ary.shape\n",
    "    if ary.ndim == 3:\n",
    "        w, x, y = ary.shape\n",
    "        z = 1\n",
    "    if ary.ndim == 2:\n",
    "        w, x = ary.shape\n",
    "        y, z, = 1, 1\n",
    "    if ndim==4:\n",
    "        return ary.reshape((w, x, y, z))\n",
    "    if ndim==3:\n",
    "        return ary.reshape((w, x, y))\n",
    "    if ndim==2:\n",
    "        return ary.reshape((w, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 32, 32, 1) (10000, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "dataset = 'mnist'\n",
    "if dataset == 'mnist':\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = np.pad(x_train, ((0,0), (2,2), (2,2)), 'edge')\n",
    "    x_test = np.pad(x_test, ((0,0), (2,2), (2,2)), 'edge')\n",
    "    \n",
    "if x_train.ndim == 3:\n",
    "    s = x_train.shape\n",
    "    x_train = x_train.reshape((len(x_train), s[1], s[2], 1))\n",
    "    x_test = x_test.reshape((len(x_test), s[1], s[2], 1))\n",
    "original_img_size = x_train[0].shape\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape((x_train.shape[0],) + original_img_size)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape((x_test.shape[0],) + original_img_size)\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "y_train_oh = np.asarray(np_utils.to_categorical(y_train), 'float32')\n",
    "y_test_oh = np.asarray(np_utils.to_categorical(y_test), 'float32')\n",
    "\n",
    "print(y_train_oh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "          \r",
      "5500/|/[loss: nan]   9%|| 5500/60000 [00:24<00:31, 1755.83it/s]"
     ]
    }
   ],
   "source": [
    "class Autoencoder(models_basic.SaveableModel):\n",
    "    \"\"\"\n",
    "    Base class for all-purpose autoencoder. VAE, CNN-AE, etc will be built off of this.\n",
    "\n",
    "    Input -> Encoder -> Z Latent Vector -> Decoder -> Output\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_shape=(28, 28, 1),\n",
    "                 latent_dim=2,  # Size of the encoded vector\n",
    "                 batch_size=100, # size of minibatch\n",
    "                 compile_decoder=False # create the decoder. Not necessary for every use case\n",
    "                 ):\n",
    "        self.model = None\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.batch_size = batch_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.compile_decoder = compile_decoder\n",
    "        assert K_backend.image_dim_ordering() == 'tf', 'Cannot support Theano ordering! Use TF ordering! #tensorflowmasterrace'\n",
    "\n",
    "        # input image dimensions\n",
    "        self.input_shape = input_shape\n",
    "        # self.data_shape = input_shape[1:] # Shape of a single sample\n",
    "        if len(input_shape) == 4:\n",
    "            self.img_rows, self.img_cols, self.img_stacks, self.img_chns = input_shape\n",
    "        elif len(input_shape) == 3:\n",
    "            self.img_rows, self.img_cols, self.img_chns = input_shape\n",
    "        elif len(input_shape) == 2:\n",
    "            self.img_rows, self.img_cols = input_shape\n",
    "            self.img_chns = 1\n",
    "        elif len(input_shape) == 1:\n",
    "            self.img_rows = input_shape[0]  # todo: test this\n",
    "        else:\n",
    "            raise ValueError(\"Invalid input shape: {}\".format(input_shape))\n",
    "\n",
    "    def rollup_decoder(self, z, z_input, layers_list):\n",
    "        \"\"\"\n",
    "        Takes a list of Keras layers and returns the decoder back-half and the standalone decoder model\n",
    "        :param z: Layer corresponding to the latent space vector\n",
    "        :param z_input: Layer corresponding to the decoder input\n",
    "        :param layers_list: List of layers to roll up\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        ae = AE_Dec()\n",
    "        dc = AE_Dec()\n",
    "        last_ae = z\n",
    "        last_dc = z_input\n",
    "        for i, layer in enumerate(layers_list):\n",
    "            #             if i ==0:\n",
    "            last_ae = layer(last_ae)\n",
    "            if self.compile_decoder:\n",
    "                last_dc = layer(last_dc)\n",
    "        return last_ae, last_dc\n",
    "\n",
    "\n",
    "class AE_Dec(object):\n",
    "    \"\"\"\n",
    "    Dummy object for reasons I can't remember. This may be deprecated.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class VAE(Autoencoder):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_shape=(28, 28, 1),\n",
    "                 latent_dim=2,  # Size of the encoded vector\n",
    "                 batch_size=100,  # size of minibatch\n",
    "                 epsilon_std=1.0, # This is the stddev for our normal-dist sampling of the latent vector\n",
    "                 compile_decoder=False\n",
    "                 ):\n",
    "        super().__init__(input_shape=input_shape, latent_dim=latent_dim, batch_size=batch_size,\n",
    "                         compile_decoder=compile_decoder)\n",
    "        # Necessary to instantiate this as instance variables such that they can be passed to the loss function (internally), since loss functions are\n",
    "        # all of the form lossfn(y_true, y_pred)\n",
    "        self.epsilon_std = epsilon_std\n",
    "        self.z_mean = Dense(latent_dim)\n",
    "        self.z_log_var = Dense(latent_dim)\n",
    "\n",
    "\n",
    "\n",
    "    def sampling(self, args):\n",
    "        \"\"\"\n",
    "        This is what makes the variational technique happen.\n",
    "        :param args:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Forging our latent vector from the reparameterized mean and std requires some sampling trickery\n",
    "        # that admittedly I do not understand in the slightest at this point in time\n",
    "        z_mean, z_log_var = args\n",
    "        batch_size = self.batch_size\n",
    "        epsilon = K_backend.random_normal(shape=(batch_size, self.latent_dim),\n",
    "                                          mean=0., stddev=self.epsilon_std)\n",
    "        # We return z_mean + epsilon*sigma^2. Not sure why we use log var\n",
    "        # Basically, create a random variable vector from the distribution\n",
    "        # We are learning a distribution (mu, var) which represents the input\n",
    "        return z_mean + K_backend.exp(z_log_var) * epsilon\n",
    "\n",
    "    def vae_loss(self, x, x_prime):\n",
    "        \"\"\"\n",
    "        Custom loss function for VAE. Uses Kullback-Leibler divergence.\n",
    "\n",
    "        Notes from fchollet: binary_crossentropy expects a shape (batch_size, dim) for x and x_prime,\n",
    "        so we MUST flatten these!\n",
    "        :param x:\n",
    "        :param x_prime:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        x = K_backend.flatten(x)\n",
    "        x_prime = K_backend.flatten(x_prime)\n",
    "        shape_coef = np.product(self.input_shape)\n",
    "        xent_loss = shape_coef * objectives.binary_crossentropy(x, x_prime)\n",
    "        kl_loss = - 0.5 * K_backend.mean(\n",
    "            1 + self.z_log_var - K_backend.square(self.z_mean) - K_backend.exp(self.z_log_var), axis=-1)\n",
    "        return xent_loss + kl_loss\n",
    "    \n",
    "    def vae_loss_kl(self, x, x_prime):\n",
    "        x = K_backend.flatten(x)\n",
    "        x_prime = K_backend.flatten(x_prime)\n",
    "        shape_coef = np.product(self.input_shape)\n",
    "        xent_loss = shape_coef * objectives.binary_crossentropy(x, x_prime)\n",
    "        kl_loss = - 0.5 * K_backend.mean(\n",
    "            1 + self.z_log_var - K_backend.square(self.z_mean) - K_backend.exp(self.z_log_var), axis=-1)\n",
    "        return xent_loss + kl_loss\n",
    "\n",
    "\n",
    "class DiscVAE(VAE):\n",
    "    \"\"\" Covolutional Discriminative VAE\n",
    "     Discriminative Regularization for Generative Models\n",
    "     \"\"\"\n",
    "    coef_xent = 1.\n",
    "    coef_kl = 1.\n",
    "    coef_disc = 0.1\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_shape=(64, 64, 3),\n",
    "                 latent_dim=256,  # Size of the encoded vector\n",
    "                 n_classes=10,  # number of classes in dataset\n",
    "                 batch_size=100,  # size of minibatch\n",
    "                 n_stacks=3,  # Number of convolayers to stack, this boosts performance of the network dramatically\n",
    "                 intermediate_dim=256,  # Size of the dense layer after convs\n",
    "                 n_filters=64,  # Number of filters in the first layer\n",
    "                 px_conv=3,  # Default convolution window size\n",
    "                 dropout_p=0.1,  # Default dropout rate\n",
    "                 epsilon_std=1.0,  # This is the stddev for our normal-dist sampling of the latent vector\n",
    "                 compile_decoder=True,\n",
    "                 ):\n",
    "\n",
    "        # This is my original crossfire network, and it works. As such, it has apprentice marks all over\n",
    "        # Reconstructing as-is before tinkering\n",
    "        # Based heavily on https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder_deconv.py\n",
    "        # and https://groups.google.com/forum/#!msg/keras-users/iBp3Ngxll3k/_GbY4nqNCQAJ\n",
    "\n",
    "        super().__init__(input_shape=input_shape, latent_dim=latent_dim, batch_size=batch_size, epsilon_std=epsilon_std,\n",
    "                         compile_decoder=compile_decoder)\n",
    "        self.new_model(input_shape=input_shape, latent_dim=latent_dim, n_stacks=n_stacks)\n",
    "        \n",
    "    def rollup_disc(self, z, z_input, layers_list, disc_list):\n",
    "        \"\"\"\n",
    "        Takes a list of Keras layers and returns the decoder back-half and the standalone decoder model\n",
    "        :param z: Layer corresponding to the latent space vector\n",
    "        :param z_input: Layer corresponding to the decoder input\n",
    "        :param layers_list: List of layers to roll up\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        last_ae = z\n",
    "        last_disc = z\n",
    "        last_dc = z_input\n",
    "        for i, layer in enumerate(layers_list):\n",
    "            #             if i ==0:\n",
    "            last_ae = layer(last_ae)\n",
    "            last_disc = layer(last_disc)\n",
    "            if self.compile_decoder:\n",
    "                last_dc = layer(last_dc)\n",
    "        for layer in disc_list:\n",
    "            last_disc = layer(last_disc)\n",
    "        return last_ae, last_dc, last_disc\n",
    "\n",
    "        \n",
    "    def discvae_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param x:\n",
    "        :param y: category\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        x = K_backend.flatten(self.x_in)\n",
    "        x_prime = K_backend.flatten(self.x_prime)\n",
    "        shape_coef = np.product(self.input_shape)\n",
    "        xent_loss = shape_coef * objectives.binary_crossentropy(x, x_prime)\n",
    "        disc_loss = objectives.categorical_crossentropy(y_true, y_pred)\n",
    "        kl_loss = - 0.5 * K_backend.mean(\n",
    "            1 + self.z_log_var - K_backend.square(self.z_mean) - K_backend.exp(self.z_log_var), axis=-1)\n",
    "        return xent_loss + kl_loss + disc_loss\n",
    "\n",
    "\n",
    "    def new_model(self,\n",
    "                  input_shape=(64,64,3),\n",
    "                  latent_dim=512,\n",
    "                  kern=3,\n",
    "                  n_classes=10,\n",
    "                  n_filtersX=32,\n",
    "                  n_stacks=4,\n",
    "                  dropout_p=0.2,\n",
    "                  intermediate_dim=512):\n",
    "        # Convolutional frontend filters as per typical convonets\n",
    "        print(self.input_shape)\n",
    "\n",
    "        core_width = input_shape[0] // 2**n_stacks # this will probably break horribly for shapes not power of 2\n",
    "        n_filtersZ = n_filtersX * 2**(n_stacks-1)\n",
    "\n",
    "        x_in = Input(self.input_shape, name='main_input')\n",
    "        self.x_in = x_in\n",
    "        stack = x_in # bit of a hack so we can use a for loop here\n",
    "        \n",
    "        enc_list = [BatchNormalization()] # stack of encoder layers\n",
    "        disc_list = [BatchNormalization()] # stack of discriminator layers\n",
    "        res_list = [] # stack for linking residual layers\n",
    "        \n",
    "        \n",
    "        # pre-stack, don't downsize\n",
    "#         enc_list.append(Conv2D(n_filtersX, (kern, kern), padding='same', activation='relu', name='conv_{}a'.format('p')))\n",
    "#         enc_list.append(BatchNormalization())\n",
    "#         enc_list.append(Activation('relu'))\n",
    "#         enc_list.append(Dropout(dropout_p))\n",
    "#         enc_list.append(Conv2D(n_filtersX, (kern, kern), padding='same', activation='relu', name='conv_{}b'.format('p')))\n",
    "#         enc_list.append(BatchNormalization())\n",
    "#         enc_list.append(Activation('relu'))\n",
    "#         enc_list.append(Conv2D(n_filtersX, (kern, kern), padding='same', activation='relu', name='conv_{}c'.format('p')))\n",
    "\n",
    "\n",
    "        # -> CONV/FC -> BatchNorm -> ReLu(or other activation) -> Dropout -> CONV/FC ->\n",
    "        for i in range(n_stacks):\n",
    "            enc_list.append(Conv2D(n_filtersX * 2 ** i, (kern, kern), padding='same', activation='relu', name='conv_{}a'.format(i)))\n",
    "            enc_list.append(BatchNormalization())\n",
    "            enc_list.append(Activation('relu'))\n",
    "            enc_list.append(Dropout(dropout_p))\n",
    "            enc_list.append(Conv2D(n_filtersX * 2 ** i, (kern, kern), padding='same', activation='relu', name='conv_{}b'.format(i)))\n",
    "            enc_list.append(BatchNormalization())\n",
    "            enc_list.append(Activation('relu'))\n",
    "#             enc_list.append(Dropout(dropout_p)) # dropout of 0.2 here causes network to NaN-out. Weird. \n",
    "            enc_list.append(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "            disc_list.append(Conv2D(n_filtersX * 2 ** i, (kern, kern), padding='same', activation='relu', name='conv_D{}a'.format(i)))\n",
    "            disc_list.append(BatchNormalization())\n",
    "            disc_list.append(Activation('relu'))\n",
    "            disc_list.append(Dropout(dropout_p))\n",
    "            disc_list.append(Conv2D(n_filtersX * 2 ** i, (kern, kern), padding='same', activation='relu', name='conv_D{}b'.format(i)))\n",
    "            disc_list.append(BatchNormalization())\n",
    "            disc_list.append(Activation('relu'))\n",
    "#             disc_list.append(Dropout(dropout_p)) # dropout of 0.2 here causes network to NaN-out. Weird. \n",
    "            disc_list.append(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "\n",
    "        enc_tensors = [x_in]\n",
    "        res_tensors = []\n",
    "        for layer in enc_list:\n",
    "#             stack = layer(stack) # ROLL OUT! connect up all the primary layers\n",
    "            enc_tensors.append(layer(enc_tensors[-1]))\n",
    "        stack = enc_tensors[-1]\n",
    "            \n",
    "        for i in range(5, len(enc_list), 8):\n",
    "            conv_layer = Conv2D(32, (kern, kern), strides=(2,2), padding='same', activation='relu')(enc_tensors[i])\n",
    "            res_tensors.append(Flatten()(conv_layer))\n",
    "            print(i, enc_list[i])\n",
    "#         res_flattened = [Flatten()(layer) for layer in res_list]\n",
    "#         res_flattened = Dense(10)(res_list[0])\n",
    "        \n",
    "\n",
    "\n",
    "        # Densely connected layer after the filters\n",
    "        flat = Flatten()(stack)\n",
    "        \n",
    "        combined = merge([flat,] + res_tensors, mode='concat')\n",
    "        hidden_1 = Dense(intermediate_dim, activation='relu', name='intermezzo')(flat)\n",
    "\n",
    "        # This is the Variational Autoencoder reparameterization trick\n",
    "        z_mean = Dense(latent_dim)(hidden_1)\n",
    "        z_log_var = Dense(latent_dim)(hidden_1)\n",
    "\n",
    "        # Make these instance vars so X-Ent can use them. Probably a better way out there\n",
    "        self.z_mean = z_mean\n",
    "        self.z_log_var = z_log_var\n",
    "\n",
    "#         # Part 2 of the reparam trick is sample from the mean-vec and std-vec (log_var). To do this, we utilize a\n",
    "#         # custom layer via Lambda class to combine the mean and log_var outputs and a custom sampling function\n",
    "#         # 'z' is our latent vector\n",
    "        z = Lambda(self.sampling, output_shape=(latent_dim,), name='latent_z')([z_mean, z_log_var])\n",
    "#         print(z.shape)\n",
    "\n",
    "#         # This marks the end of the encoding portion of the VAE\n",
    "\n",
    "#         # On to Decoder. we instantiate these layers separately so as to reuse them later\n",
    "#         # e.g. for feeding in latent-space vectors, or (presumably) inspecting output\n",
    "\n",
    "        output_shape = (None, core_width, core_width, n_filtersZ)\n",
    "\n",
    "        decoder_hidden = Dense(intermediate_dim, activation='relu')\n",
    "        decoder_upsample = Dense(n_filtersZ * core_width**2, activation='relu')\n",
    "\n",
    "\n",
    "        decoder_reshape = Reshape(output_shape[1:])\n",
    "\n",
    "        dec_list = [decoder_hidden, decoder_upsample, decoder_reshape]\n",
    "\n",
    "        for i in range(n_stacks-1):\n",
    "            dec_list.append(UpSampling2D((2,2)))\n",
    "            dec_list.append(Deconv2D(n_filtersX * 2 ** (n_stacks-i-1), (kern, kern), padding='same', activation='relu', name='deconv_{}a'.format(i)))\n",
    "            dec_list.append(BatchNormalization())\n",
    "            dec_list.append(Activation('relu'))\n",
    "#             dec_list.append(Dropout(dropout_p))\n",
    "            dec_list.append(Deconv2D(n_filtersX * 2 ** (n_stacks-i-1), (kern, kern), padding='same', activation='relu', name='deconv_{}b'.format(i)))\n",
    "            dec_list.append(BatchNormalization())\n",
    "            dec_list.append(Activation('relu'))\n",
    "#             dec_list.append(Dropout(dropout_p))\n",
    "            \n",
    "        dec_list.append(UpSampling2D((2,2)))\n",
    "        dec_list.append(Conv2D(self.input_shape[2], (kern, kern), padding='same', activation='sigmoid', name='deconv_fin')) # trick to drop down to N channels from filters\n",
    "\n",
    "        decoder_input = Input(shape=(latent_dim,))\n",
    "\n",
    "        # todo: better naming convention\n",
    "        ae, dc = self.rollup_decoder(z, decoder_input, dec_list)\n",
    "        \n",
    "#         ae, dc, disc = self.rollup_disc(z, decoder_input, dec_list, disc_list)\n",
    "#         disc = Flatten()(disc)\n",
    "#         disc = Dense(n_classes, activation='sigmoid')(disc) # classer\n",
    "#         print(type(ae), type(disc))\n",
    "#         print(disc)\n",
    "#         self.model_disc = Model(x_in, disc)\n",
    "#         self.model_disc.compile(optimizer='rmsprop', loss=self.discvae_loss) # loss=self.discvae_loss\n",
    "\n",
    "        self.x_prime = ae\n",
    "\n",
    "#         # Now we create the actual models. We also compile them automatically, this could be isolated later\n",
    "#         # Primary model - VAE\n",
    "        self.model_ae = Model(x_in, ae)\n",
    "        self.model_ae.compile(optimizer='rmsprop', loss=self.vae_loss)\n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "        self.model = self.model_ae\n",
    "        \n",
    "\n",
    "\n",
    "        # build a model to project inputs on the latent space\n",
    "#         self.encoder = Model(x_in, self.z_mean)\n",
    "#         self.foo = Model(x_in, decoder_reshape)\n",
    "        if 0:# self.compile_decoder:\n",
    "            # reconstruct the digit pictures from latent space\n",
    "            self.decoder = Model(decoder_input, dc)\n",
    "\n",
    "    def fit_crossmodel(self, x_dict, y_dict, batch_size=None, nb_epoch=10, verbose=1, callbacks=[],\n",
    "                       validation_split=0.,\n",
    "                       validation_data=None, shuffle=True, class_weight=None, sample_weight=None):\n",
    "        pass\n",
    "        #     callbacks_history = self.crossmodel.fit(x_dict, y_dict, batch_size, nb_epoch, verbose, callbacks,\n",
    "        #                                             validation_split,\n",
    "        #                                             validation_data, shuffle, class_weight, sample_weight)\n",
    "        #     return callbacks_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 1)\n",
      "5 <keras.layers.convolutional.Conv2D object at 0x7f18f98032e8>\n",
      "13 <keras.layers.convolutional.Conv2D object at 0x7f18f9803ef0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:278: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/keras/legacy/layers.py:456: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 32, 32, 1)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_791 (BatchNo (None, 32, 32, 1)     4                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_0a (Conv2D)                 (None, 32, 32, 32)    320                                          \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_793 (BatchNo (None, 32, 32, 32)    128                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_703 (Activation)      (None, 32, 32, 32)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_284 (Dropout)            (None, 32, 32, 32)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_0b (Conv2D)                 (None, 32, 32, 32)    9248                                         \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_794 (BatchNo (None, 32, 32, 32)    128                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_704 (Activation)      (None, 32, 32, 32)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_277 (MaxPooling2D) (None, 16, 16, 32)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_1a (Conv2D)                 (None, 16, 16, 64)    18496                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_797 (BatchNo (None, 16, 16, 64)    256                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_707 (Activation)      (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_286 (Dropout)            (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_1b (Conv2D)                 (None, 16, 16, 64)    36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_798 (BatchNo (None, 16, 16, 64)    256                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_708 (Activation)      (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_279 (MaxPooling2D) (None, 8, 8, 64)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_138 (Flatten)            (None, 4096)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "intermezzo (Dense)               (None, 512)           2097664                                      \n",
      "____________________________________________________________________________________________________\n",
      "dense_234 (Dense)                (None, 3)             1539                                         \n",
      "____________________________________________________________________________________________________\n",
      "dense_235 (Dense)                (None, 3)             1539                                         \n",
      "____________________________________________________________________________________________________\n",
      "latent_z (Lambda)                (None, 3)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_236 (Dense)                (None, 512)           2048                                         \n",
      "____________________________________________________________________________________________________\n",
      "dense_237 (Dense)                (None, 4096)          2101248                                      \n",
      "____________________________________________________________________________________________________\n",
      "reshape_33 (Reshape)             (None, 8, 8, 64)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_103 (UpSampling2D) (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "deconv_0a (Conv2DTranspose)      (None, 16, 16, 64)    36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_801 (BatchNo (None, 16, 16, 64)    256                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_711 (Activation)      (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "deconv_0b (Conv2DTranspose)      (None, 16, 16, 64)    36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_802 (BatchNo (None, 16, 16, 64)    256                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_712 (Activation)      (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_104 (UpSampling2D) (None, 32, 32, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "deconv_fin (Conv2D)              (None, 32, 32, 1)     577                                          \n",
      "====================================================================================================\n",
      "Total params: 4,344,747.0\n",
      "Trainable params: 4,344,105.0\n",
      "Non-trainable params: 642.0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import autoencoder\n",
    "from imp import reload\n",
    "reload(autoencoder)\n",
    "\n",
    "# aeclass = autoencoder.VAE_MNIST_0(batch_size=100, n_stacks=0, compile_decoder=False)\n",
    "aeclass = DiscVAE(input_shape=x_train[0].shape, latent_dim=3, n_stacks=2, n_classes=y_train_oh.shape[1])\n",
    "print(aeclass.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keras.utils.vis_utils.plot_model(aeclass.model, 'mymodel.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d50e32a249435badf3197d366daf48"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3879a892a77441aaa3e2b5b87047cced"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04914a9f8ce14a6397c24524fc2bbc0a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f18f6d61400>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aeclass.model.fit(x_train, x_train, batch_size=100, nb_epoch=2, verbose=0, callbacks=[TQDMNotebookCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# aeclass.model_ae.fit(x_train, x_train, batch_size=100, nb_epoch=2, verbose=0, callbacks=[TQDMNotebookCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pp = aeclass.model_ae.predict(redim(x_test[:200]), batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.996078 0.930387\n",
      "0.0 6.24419e-11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f18f3f7c208>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f18fa978518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 113\n",
    "if x_test.shape[-1] == 1:\n",
    "    s = pp.shape\n",
    "    pp = pp.reshape((s[0], s[1], s[2]))\n",
    "    x_test = x_test.reshape((len(x_test), s[1], s[2]))\n",
    "    \n",
    "print(np.amax(x_test[n]), np.amax(pp[n]))\n",
    "print(np.amin(x_test[n]), np.amin(pp[n]))\n",
    "plt.imshow(x_test[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f18f2758d68>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f18f8ff88d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(redim(pp[n], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''This script demonstrates how to build a variational autoencoder\n",
    "with Keras and deconvolution layers.\n",
    "\n",
    "Reference: \"Auto-Encoding Variational Bayes\" https://arxiv.org/abs/1312.6114\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape\n",
    "from keras.layers import Convolution2D, Deconvolution2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols, img_chns = 28, 28, 1\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 64\n",
    "# convolution kernel size\n",
    "nb_conv = 3\n",
    "\n",
    "batch_size = 100\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    original_img_size = (img_chns, img_rows, img_cols)\n",
    "else:\n",
    "    original_img_size = (img_rows, img_cols, img_chns)\n",
    "latent_dim = 2\n",
    "intermediate_dim = 128\n",
    "epsilon_std = 0.01\n",
    "nb_epoch = 2\n",
    "\n",
    "x = Input(batch_shape=(batch_size,) + original_img_size)\n",
    "conv_1 = Convolution2D(img_chns, 2, 2, border_mode='same', activation='relu')(x)\n",
    "conv_2 = Convolution2D(nb_filters, 2, 2,\n",
    "                       border_mode='same', activation='relu',\n",
    "                       subsample=(2, 2))(conv_1)\n",
    "conv_3 = Convolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                       border_mode='same', activation='relu',\n",
    "                       subsample=(1, 1))(conv_2)\n",
    "conv_4 = Convolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                       border_mode='same', activation='relu',\n",
    "                       subsample=(1, 1))(conv_3)\n",
    "flat = Flatten()(conv_4)\n",
    "hidden = Dense(intermediate_dim, activation='relu')(flat)\n",
    "\n",
    "z_mean = Dense(latent_dim)(hidden)\n",
    "z_log_var = Dense(latent_dim)(hidden)\n",
    "\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim),\n",
    "                              mean=0., std=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "# so you could write `Lambda(sampling)([z_mean, z_log_var])`\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# we instantiate these layers separately so as to reuse them later\n",
    "decoder_hid = Dense(intermediate_dim, activation='relu')\n",
    "decoder_upsample = Dense(nb_filters * 14 * 14, activation='relu')\n",
    "\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    output_shape = (batch_size, nb_filters, 14, 14)\n",
    "else:\n",
    "    output_shape = (batch_size, 14, 14, nb_filters)\n",
    "\n",
    "decoder_reshape = Reshape(output_shape[1:])\n",
    "decoder_deconv_1 = Deconvolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                                   output_shape,\n",
    "                                   border_mode='same',\n",
    "                                   subsample=(1, 1),\n",
    "                                   activation='relu')\n",
    "decoder_deconv_2 = Deconvolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                                   output_shape,\n",
    "                                   border_mode='same',\n",
    "                                   subsample=(1, 1),\n",
    "                                   activation='relu')\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    output_shape = (batch_size, nb_filters, 29, 29)\n",
    "else:\n",
    "    output_shape = (batch_size, 29, 29, nb_filters)\n",
    "decoder_deconv_3_upsamp = Deconvolution2D(nb_filters, 2, 2,\n",
    "                                          output_shape,\n",
    "                                          border_mode='valid',\n",
    "                                          subsample=(2, 2),\n",
    "                                          activation='relu')\n",
    "decoder_mean_squash = Convolution2D(img_chns, 2, 2,\n",
    "                                    border_mode='valid',\n",
    "                                    activation='sigmoid')\n",
    "\n",
    "hid_decoded = decoder_hid(z)\n",
    "up_decoded = decoder_upsample(hid_decoded)\n",
    "reshape_decoded = decoder_reshape(up_decoded)\n",
    "deconv_1_decoded = decoder_deconv_1(reshape_decoded)\n",
    "deconv_2_decoded = decoder_deconv_2(deconv_1_decoded)\n",
    "x_decoded_relu = decoder_deconv_3_upsamp(deconv_2_decoded)\n",
    "x_prime_squash = decoder_mean_squash(x_decoded_relu)\n",
    "\n",
    "def vae_loss(x, x_prime):\n",
    "    # NOTE: binary_crossentropy expects a batch_size by dim\n",
    "    # for x and x_prime, so we MUST flatten these!\n",
    "    x = K.flatten(x)\n",
    "    x_prime = K.flatten(x_prime)\n",
    "    xent_loss = img_rows * img_cols * objectives.binary_crossentropy(x, x_prime)\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "vae = Model(x, x_prime_squash)\n",
    "vae.compile(optimizer='rmsprop', loss=vae_loss)\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train the VAE on MNIST digits\n",
    "(x_train, _), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train[:, :28, :28]\n",
    "x_test = x_test[:, :28, :28]\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape((x_train.shape[0],) + original_img_size)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape((x_test.shape[0],) + original_img_size)\n",
    "\n",
    "print('x_train.shape:', x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vae.fit(x1, x1,\n",
    "        shuffle=True,\n",
    "        nb_epoch=nb_epoch,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, x_test), \n",
    "       verbose=0, callbacks=[TQDMNotebookCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# build a model to project inputs on the latent space\n",
    "encoder = Model(x, z_mean)\n",
    "\n",
    "# display a 2D plot of the digit classes in the latent space\n",
    "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# build a digit generator that can sample from the learned distribution\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_hid_decoded = decoder_hid(decoder_input)\n",
    "_up_decoded = decoder_upsample(_hid_decoded)\n",
    "_reshape_decoded = decoder_reshape(_up_decoded)\n",
    "_deconv_1_decoded = decoder_deconv_1(_reshape_decoded)\n",
    "_deconv_2_decoded = decoder_deconv_2(_deconv_1_decoded)\n",
    "_x_decoded_relu = decoder_deconv_3_upsamp(_deconv_2_decoded)\n",
    "_x_prime_squash = decoder_mean_squash(_x_decoded_relu)\n",
    "generator = Model(decoder_input, _x_prime_squash)\n",
    "\n",
    "# display a 2D manifold of the digits\n",
    "n = 15  # figure with 15x15 digits\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n, x_train.shape[3]))\n",
    "# we will sample n points within [-15, 15] standard deviations\n",
    "grid_x = np.linspace(-15, 15, n)\n",
    "grid_y = np.linspace(-15, 15, n)\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        z_sample = np.tile(z_sample, batch_size).reshape(batch_size, 2)\n",
    "        x_decoded = generator.predict(z_sample, batch_size=batch_size)\n",
    "        digit = x_decoded[0].reshape((digit_size, digit_size, -1))\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
