{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "# os.environ['KERAS_BACKEND'] = 'theano'\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "from tensorflow import test\n",
    "test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10, mnist\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "if x_train.ndim == 3:\n",
    "    s = x_train.shape\n",
    "    x_train = x_train.reshape((len(x_train), s[1], s[2], 1))\n",
    "    x_test = x_test.reshape((len(x_test), s[1], s[2], 1))\n",
    "original_img_size = x_train[0].shape\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape((x_train.shape[0],) + original_img_size)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape((x_test.shape[0],) + original_img_size)\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 10)\n"
     ]
    }
   ],
   "source": [
    "y_train_oh = np_utils.to_categorical(y_train)\n",
    "y_test_oh = np_utils.to_categorical(y_test)\n",
    "\n",
    "print(y_train_oh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal, ndimage, misc\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape, BatchNormalization, Dropout, GaussianNoise\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Conv2D, Deconv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.layers import Conv3D, UpSampling3D, MaxPooling3D\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras import backend as K_backend\n",
    "from keras import objectives\n",
    "\n",
    "import models_basic\n",
    "\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "from autoencoder import VAE_MNIST_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Autoencoder(models_basic.SaveableModel):\n",
    "    \"\"\"\n",
    "    Base class for all-purpose autoencoder. VAE, CNN-AE, etc will be built off of this.\n",
    "\n",
    "    Input -> Encoder -> Z Latent Vector -> Decoder -> Output\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_shape=(28, 28, 1),\n",
    "                 latent_dim=2,  # Size of the encoded vector\n",
    "                 batch_size=100, # size of minibatch\n",
    "                 compile_decoder=False # create the decoder. Not necessary for every use case\n",
    "                 ):\n",
    "        self.model = None\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.batch_size = batch_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.compile_decoder = compile_decoder\n",
    "        assert K_backend.image_dim_ordering() == 'tf', 'Cannot support Theano ordering! Use TF ordering! #tensorflowmasterrace'\n",
    "\n",
    "        # input image dimensions\n",
    "        self.input_shape = input_shape\n",
    "        # self.data_shape = input_shape[1:] # Shape of a single sample\n",
    "        if len(input_shape) == 4:\n",
    "            self.img_rows, self.img_cols, self.img_stacks, self.img_chns = input_shape\n",
    "        elif len(input_shape) == 3:\n",
    "            self.img_rows, self.img_cols, self.img_chns = input_shape\n",
    "        elif len(input_shape) == 2:\n",
    "            self.img_rows, self.img_cols = input_shape\n",
    "            self.img_chns = 1\n",
    "        elif len(input_shape) == 1:\n",
    "            self.img_rows = input_shape[0]  # todo: test this\n",
    "        else:\n",
    "            raise ValueError(\"Invalid input shape: {}\".format(input_shape))\n",
    "\n",
    "    def rollup_decoder(self, z, z_input, layers_list):\n",
    "        \"\"\"\n",
    "        Takes a list of Keras layers and returns the decoder back-half and the standalone decoder model\n",
    "        :param z: Layer corresponding to the latent space vector\n",
    "        :param z_input: Layer corresponding to the decoder input\n",
    "        :param layers_list: List of layers to roll up\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        ae = AE_Dec()\n",
    "        dc = AE_Dec()\n",
    "        last_ae = z\n",
    "        last_dc = z_input\n",
    "        for i, layer in enumerate(layers_list):\n",
    "            #             if i ==0:\n",
    "            last_ae = layer(last_ae)\n",
    "            if self.compile_decoder:\n",
    "                last_dc = layer(last_dc)\n",
    "        return last_ae, last_dc\n",
    "\n",
    "\n",
    "class AE_Dec(object):\n",
    "    \"\"\"\n",
    "    Dummy object for reasons I can't remember. This may be deprecated.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class VAE(Autoencoder):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_shape=(28, 28, 1),\n",
    "                 latent_dim=2,  # Size of the encoded vector\n",
    "                 batch_size=100,  # size of minibatch\n",
    "                 epsilon_std=1.0, # This is the stddev for our normal-dist sampling of the latent vector\n",
    "                 compile_decoder=False\n",
    "                 ):\n",
    "        super().__init__(input_shape=input_shape, latent_dim=latent_dim, batch_size=batch_size,\n",
    "                         compile_decoder=compile_decoder)\n",
    "        # Necessary to instantiate this as instance variables such that they can be passed to the loss function (internally), since loss functions are\n",
    "        # all of the form lossfn(y_true, y_pred)\n",
    "        self.epsilon_std = epsilon_std\n",
    "        self.z_mean = Dense(latent_dim)\n",
    "        self.z_log_var = Dense(latent_dim)\n",
    "\n",
    "\n",
    "\n",
    "    def sampling(self, args):\n",
    "        \"\"\"\n",
    "        This is what makes the variational technique happen.\n",
    "        :param args:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Forging our latent vector from the reparameterized mean and std requires some sampling trickery\n",
    "        # that admittedly I do not understand in the slightest at this point in time\n",
    "        z_mean, z_log_var = args\n",
    "        batch_size = self.batch_size\n",
    "        epsilon = K_backend.random_normal(shape=(batch_size, self.latent_dim),\n",
    "                                          mean=0., stddev=self.epsilon_std)\n",
    "        # We return z_mean + epsilon*sigma^2. Not sure why we use log var\n",
    "        # Basically, create a random variable vector from the distribution\n",
    "        # We are learning a distribution (mu, var) which represents the input\n",
    "        return z_mean + K_backend.exp(z_log_var) * epsilon\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean):\n",
    "        \"\"\"\n",
    "        Custom loss function for VAE. Uses Kullback-Leibler divergence.\n",
    "\n",
    "        Notes from fchollet: binary_crossentropy expects a shape (batch_size, dim) for x and x_decoded_mean,\n",
    "        so we MUST flatten these!\n",
    "        :param x:\n",
    "        :param x_decoded_mean:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        x = K_backend.flatten(x)\n",
    "        x_decoded_mean = K_backend.flatten(x_decoded_mean)\n",
    "        shape_coef = np.product(self.input_shape)\n",
    "        xent_loss = shape_coef * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "        kl_loss = - 0.5 * K_backend.mean(\n",
    "            1 + self.z_log_var - K_backend.square(self.z_mean) - K_backend.exp(self.z_log_var), axis=-1)\n",
    "        return xent_loss + kl_loss\n",
    "\n",
    "\n",
    "class DiscVAE(VAE):\n",
    "    \"\"\" Covolutional Discriminative VAE\n",
    "     Discriminative Regularization for Generative Models\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_shape=(64, 64, 3),\n",
    "                 latent_dim=256,  # Size of the encoded vector\n",
    "                 n_classes=10,  # number of classes in dataset\n",
    "                 batch_size=100,  # size of minibatch\n",
    "                 n_stacks=3,  # Number of convolayers to stack, this boosts performance of the network dramatically\n",
    "                 intermediate_dim=256,  # Size of the dense layer after convs\n",
    "                 n_filters=64,  # Number of filters in the first layer\n",
    "                 px_conv=3,  # Default convolution window size\n",
    "                 dropout_p=0.1,  # Default dropout rate\n",
    "                 epsilon_std=1.0,  # This is the stddev for our normal-dist sampling of the latent vector\n",
    "                 compile_decoder=True,\n",
    "                 ):\n",
    "\n",
    "        # This is my original crossfire network, and it works. As such, it has apprentice marks all over\n",
    "        # Reconstructing as-is before tinkering\n",
    "        # Based heavily on https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder_deconv.py\n",
    "        # and https://groups.google.com/forum/#!msg/keras-users/iBp3Ngxll3k/_GbY4nqNCQAJ\n",
    "\n",
    "        super().__init__(input_shape=input_shape, latent_dim=latent_dim, batch_size=batch_size, epsilon_std=epsilon_std,\n",
    "                         compile_decoder=compile_decoder)\n",
    "        self.new_model(input_shape=input_shape, latent_dim=latent_dim, n_stacks=n_stacks)\n",
    "        \n",
    "    def rollup_disc(self, z, z_input, layers_list, disc_list):\n",
    "        \"\"\"\n",
    "        Takes a list of Keras layers and returns the decoder back-half and the standalone decoder model\n",
    "        :param z: Layer corresponding to the latent space vector\n",
    "        :param z_input: Layer corresponding to the decoder input\n",
    "        :param layers_list: List of layers to roll up\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        last_ae = z\n",
    "        last_disc = z\n",
    "        last_dc = z_input\n",
    "        for i, layer in enumerate(layers_list):\n",
    "            #             if i ==0:\n",
    "            last_ae = layer(last_ae)\n",
    "            last_disc = layer(last_disc)\n",
    "            if self.compile_decoder:\n",
    "                last_dc = layer(last_dc)\n",
    "        for layer in disc_list:\n",
    "            last_disc = layer(last_disc)\n",
    "        return last_ae, last_dc, last_disc\n",
    "\n",
    "        \n",
    "    def discvae_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param x:\n",
    "        :param y: category\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        x = K_backend.flatten(self.x_in)\n",
    "        x_decoded_mean = K_backend.flatten(self.x_prime)\n",
    "        shape_coef = np.product(self.input_shape)\n",
    "        xent_loss = shape_coef * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "        disc_loss = objectives.categorical_crossentropy(y_true, y_pred)\n",
    "        kl_loss = - 0.5 * K_backend.mean(\n",
    "            1 + self.z_log_var - K_backend.square(self.z_mean) - K_backend.exp(self.z_log_var), axis=-1)\n",
    "        return xent_loss + kl_loss + disc_loss\n",
    "\n",
    "\n",
    "    def new_model(self,\n",
    "                  input_shape=(64,64,3),\n",
    "                  latent_dim=256,\n",
    "                  kern=3,\n",
    "                  n_classes=10,\n",
    "                  n_filtersX=32,\n",
    "                  n_stacks=4,\n",
    "                  dropout_p=0.2,\n",
    "                  intermediate_dim=256):\n",
    "        # Convolutional frontend filters as per typical convonets\n",
    "        print(self.input_shape)\n",
    "\n",
    "        core_width = input_shape[0] // 2**n_stacks # this will probably break horribly for shapes not power of 2\n",
    "        n_filtersZ = n_filtersX * 2**(n_stacks-1)\n",
    "\n",
    "        x_in = Input(self.input_shape, name='main_input')\n",
    "        self.x_in = x_in\n",
    "        stack = x_in # bit of a hack so we can use a for loop here\n",
    "        \n",
    "        enc_list = [BatchNormalization()]\n",
    "        disc_list = [BatchNormalization()]\n",
    "\n",
    "        # -> CONV/FC -> BatchNorm -> ReLu(or other activation) -> Dropout -> CONV/FC ->\n",
    "        for i in range(n_stacks):\n",
    "            enc_list.append(Conv2D(n_filtersX * 2 ** i, (kern, kern), padding='same', activation='relu', name='conv_{}a'.format(i)))\n",
    "            enc_list.append(BatchNormalization())\n",
    "            enc_list.append(Activation('relu'))\n",
    "            enc_list.append(Dropout(dropout_p))\n",
    "            enc_list.append(Conv2D(n_filtersX * 2 ** i, (kern, kern), padding='same', activation='relu', name='conv_{}b'.format(i)))\n",
    "            enc_list.append(BatchNormalization())\n",
    "            enc_list.append(Activation('relu'))\n",
    "#             enc_list.append(Dropout(dropout_p)) # dropout of 0.2 here causes network to NaN-out. Weird. \n",
    "            enc_list.append(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "            disc_list.append(Conv2D(n_filtersX * 2 ** i, (kern, kern), padding='same', activation='relu', name='conv_D{}a'.format(i)))\n",
    "            disc_list.append(BatchNormalization())\n",
    "            disc_list.append(Activation('relu'))\n",
    "            disc_list.append(Dropout(dropout_p))\n",
    "            disc_list.append(Conv2D(n_filtersX * 2 ** i, (kern, kern), padding='same', activation='relu', name='conv_D{}b'.format(i)))\n",
    "            disc_list.append(BatchNormalization())\n",
    "            disc_list.append(Activation('relu'))\n",
    "#             disc_list.append(Dropout(dropout_p)) # dropout of 0.2 here causes network to NaN-out. Weird. \n",
    "            disc_list.append(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "\n",
    "        for layer in enc_list:\n",
    "            stack = layer(stack) # ROLL OUT! \n",
    "\n",
    "        # Densely connected layer after the filters\n",
    "        flat = Flatten()(stack)\n",
    "        hidden_1 = Dense(intermediate_dim, activation='relu', name='intermezzo')(flat)\n",
    "\n",
    "        # This is the Variational Autoencoder reparameterization trick\n",
    "        z_mean = Dense(latent_dim)(hidden_1)\n",
    "        z_log_var = Dense(latent_dim)(hidden_1)\n",
    "\n",
    "        # Make these instance vars so X-Ent can use them. Probably a better way out there\n",
    "        self.z_mean = z_mean\n",
    "        self.z_log_var = z_log_var\n",
    "\n",
    "#         # Part 2 of the reparam trick is sample from the mean-vec and std-vec (log_var). To do this, we utilize a\n",
    "#         # custom layer via Lambda class to combine the mean and log_var outputs and a custom sampling function\n",
    "#         # 'z' is our latent vector\n",
    "        z = Lambda(self.sampling, output_shape=(latent_dim,), name='latent_z')([z_mean, z_log_var])\n",
    "#         print(z.shape)\n",
    "\n",
    "#         # This marks the end of the encoding portion of the VAE\n",
    "\n",
    "#         # On to Decoder. we instantiate these layers separately so as to reuse them later\n",
    "#         # e.g. for feeding in latent-space vectors, or (presumably) inspecting output\n",
    "\n",
    "        output_shape = (None, core_width, core_width, n_filtersZ)\n",
    "\n",
    "        decoder_hidden = Dense(intermediate_dim, activation='relu')\n",
    "        decoder_upsample = Dense(n_filtersZ * core_width**2, activation='relu')\n",
    "\n",
    "\n",
    "        decoder_reshape = Reshape(output_shape[1:])\n",
    "\n",
    "        dec_list = [decoder_hidden, decoder_upsample, decoder_reshape]\n",
    "\n",
    "        for i in range(n_stacks-1):\n",
    "            dec_list.append(UpSampling2D((2,2)))\n",
    "            dec_list.append(Deconv2D(n_filtersX * 2 ** (n_stacks-i-1), (kern, kern), padding='same', activation='relu', name='deconv_{}a'.format(i)))\n",
    "            dec_list.append(BatchNormalization())\n",
    "            dec_list.append(Activation('relu'))\n",
    "#             dec_list.append(Dropout(dropout_p))\n",
    "            dec_list.append(Deconv2D(n_filtersX * 2 ** (n_stacks-i-1), (kern, kern), padding='same', activation='relu', name='deconv_{}b'.format(i)))\n",
    "            dec_list.append(BatchNormalization())\n",
    "            dec_list.append(Activation('relu'))\n",
    "#             dec_list.append(Dropout(dropout_p))\n",
    "            \n",
    "        dec_list.append(UpSampling2D((2,2)))\n",
    "        dec_list.append(Conv2D(self.input_shape[2], (kern, kern), padding='same', activation='sigmoid', name='deconv_fin')) # trick to drop down to N channels from filters\n",
    "\n",
    "        decoder_input = Input(shape=(latent_dim,))\n",
    "\n",
    "        # todo: better naming convention\n",
    "#         ae, dc = self.rollup_decoder(z, decoder_input, dec_list)\n",
    "        ae, dc, disc = self.rollup_disc(z, decoder_input, dec_list, disc_list)\n",
    "    \n",
    "        disc = Flatten()(disc)\n",
    "        disc = Dense(n_classes, activation='sigmoid')(disc) # classer\n",
    "\n",
    "        self.x_prime = ae\n",
    "\n",
    "#         # Now we create the actual models. We also compile them automatically, this could be isolated later\n",
    "#         # Primary model - VAE\n",
    "        self.model_ae = Model(x_in, ae)\n",
    "        self.model_ae.compile(optimizer='rmsprop', loss=self.vae_loss)\n",
    "        \n",
    "\n",
    "            \n",
    "        print(type(ae), type(disc))\n",
    "        print(disc)\n",
    "        self.model = Model(x_in, disc)\n",
    "        self.model.compile(optimizer='rmsprop', loss=self.discvae_loss) # loss=self.discvae_loss\n",
    "        \n",
    "\n",
    "\n",
    "        # build a model to project inputs on the latent space\n",
    "#         self.encoder = Model(x_in, self.z_mean)\n",
    "#         self.foo = Model(x_in, decoder_reshape)\n",
    "        if 0:# self.compile_decoder:\n",
    "            # reconstruct the digit pictures from latent space\n",
    "            self.decoder = Model(decoder_input, dc)\n",
    "\n",
    "    def fit_crossmodel(self, x_dict, y_dict, batch_size=None, nb_epoch=10, verbose=1, callbacks=[],\n",
    "                       validation_split=0.,\n",
    "                       validation_data=None, shuffle=True, class_weight=None, sample_weight=None):\n",
    "        pass\n",
    "        #     callbacks_history = self.crossmodel.fit(x_dict, y_dict, batch_size, nb_epoch, verbose, callbacks,\n",
    "        #                                             validation_split,\n",
    "        #                                             validation_data, shuffle, class_weight, sample_weight)\n",
    "        #     return callbacks_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n",
      "<class 'tensorflow.python.framework.ops.Tensor'> <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Tensor(\"dense_189/Sigmoid:0\", shape=(?, 10), dtype=float32)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 32, 32, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_273 (BatchNo (None, 32, 32, 3)     12                                           \n",
      "____________________________________________________________________________________________________\n",
      "conv_0a (Conv2D)                 (None, 32, 32, 32)    896                                          \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_275 (BatchNo (None, 32, 32, 32)    128                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_241 (Activation)      (None, 32, 32, 32)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_105 (Dropout)            (None, 32, 32, 32)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_0b (Conv2D)                 (None, 32, 32, 32)    9248                                         \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_276 (BatchNo (None, 32, 32, 32)    128                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_242 (Activation)      (None, 32, 32, 32)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_89 (MaxPooling2D)  (None, 16, 16, 32)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_1a (Conv2D)                 (None, 16, 16, 64)    18496                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_279 (BatchNo (None, 16, 16, 64)    256                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_245 (Activation)      (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_107 (Dropout)            (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_1b (Conv2D)                 (None, 16, 16, 64)    36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_280 (BatchNo (None, 16, 16, 64)    256                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_246 (Activation)      (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_91 (MaxPooling2D)  (None, 8, 8, 64)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_2a (Conv2D)                 (None, 8, 8, 128)     73856                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_283 (BatchNo (None, 8, 8, 128)     512                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_249 (Activation)      (None, 8, 8, 128)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_109 (Dropout)            (None, 8, 8, 128)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_2b (Conv2D)                 (None, 8, 8, 128)     147584                                       \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_284 (BatchNo (None, 8, 8, 128)     512                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_250 (Activation)      (None, 8, 8, 128)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_93 (MaxPooling2D)  (None, 4, 4, 128)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_3a (Conv2D)                 (None, 4, 4, 256)     295168                                       \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_287 (BatchNo (None, 4, 4, 256)     1024                                         \n",
      "____________________________________________________________________________________________________\n",
      "activation_253 (Activation)      (None, 4, 4, 256)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_111 (Dropout)            (None, 4, 4, 256)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_3b (Conv2D)                 (None, 4, 4, 256)     590080                                       \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_288 (BatchNo (None, 4, 4, 256)     1024                                         \n",
      "____________________________________________________________________________________________________\n",
      "activation_254 (Activation)      (None, 4, 4, 256)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_95 (MaxPooling2D)  (None, 2, 2, 256)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_31 (Flatten)             (None, 1024)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "intermezzo (Dense)               (None, 256)           262400                                       \n",
      "____________________________________________________________________________________________________\n",
      "dense_185 (Dense)                (None, 256)           65792                                        \n",
      "____________________________________________________________________________________________________\n",
      "dense_186 (Dense)                (None, 256)           65792                                        \n",
      "____________________________________________________________________________________________________\n",
      "latent_z (Lambda)                (None, 256)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_187 (Dense)                (None, 256)           65792                                        \n",
      "____________________________________________________________________________________________________\n",
      "dense_188 (Dense)                (None, 1024)          263168                                       \n",
      "____________________________________________________________________________________________________\n",
      "reshape_31 (Reshape)             (None, 2, 2, 256)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_69 (UpSampling2D)  (None, 4, 4, 256)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "deconv_0a (Conv2DTranspose)      (None, 4, 4, 256)     590080                                       \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_291 (BatchNo (None, 4, 4, 256)     1024                                         \n",
      "____________________________________________________________________________________________________\n",
      "activation_257 (Activation)      (None, 4, 4, 256)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "deconv_0b (Conv2DTranspose)      (None, 4, 4, 256)     590080                                       \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_292 (BatchNo (None, 4, 4, 256)     1024                                         \n",
      "____________________________________________________________________________________________________\n",
      "activation_258 (Activation)      (None, 4, 4, 256)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_70 (UpSampling2D)  (None, 8, 8, 256)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "deconv_1a (Conv2DTranspose)      (None, 8, 8, 128)     295040                                       \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_293 (BatchNo (None, 8, 8, 128)     512                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_259 (Activation)      (None, 8, 8, 128)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "deconv_1b (Conv2DTranspose)      (None, 8, 8, 128)     147584                                       \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_294 (BatchNo (None, 8, 8, 128)     512                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_260 (Activation)      (None, 8, 8, 128)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_71 (UpSampling2D)  (None, 16, 16, 128)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "deconv_2a (Conv2DTranspose)      (None, 16, 16, 64)    73792                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_295 (BatchNo (None, 16, 16, 64)    256                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_261 (Activation)      (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "deconv_2b (Conv2DTranspose)      (None, 16, 16, 64)    36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_296 (BatchNo (None, 16, 16, 64)    256                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_262 (Activation)      (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_72 (UpSampling2D)  (None, 32, 32, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "deconv_fin (Conv2D)              (None, 32, 32, 3)     1731                                         \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_274 (BatchNo (None, 32, 32, 3)     12                                           \n",
      "____________________________________________________________________________________________________\n",
      "conv_D0a (Conv2D)                (None, 32, 32, 32)    896                                          \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_277 (BatchNo (None, 32, 32, 32)    128                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_243 (Activation)      (None, 32, 32, 32)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_106 (Dropout)            (None, 32, 32, 32)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_D0b (Conv2D)                (None, 32, 32, 32)    9248                                         \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_278 (BatchNo (None, 32, 32, 32)    128                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_244 (Activation)      (None, 32, 32, 32)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_90 (MaxPooling2D)  (None, 16, 16, 32)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_D1a (Conv2D)                (None, 16, 16, 64)    18496                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_281 (BatchNo (None, 16, 16, 64)    256                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_247 (Activation)      (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_108 (Dropout)            (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_D1b (Conv2D)                (None, 16, 16, 64)    36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_282 (BatchNo (None, 16, 16, 64)    256                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_248 (Activation)      (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_92 (MaxPooling2D)  (None, 8, 8, 64)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_D2a (Conv2D)                (None, 8, 8, 128)     73856                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_285 (BatchNo (None, 8, 8, 128)     512                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_251 (Activation)      (None, 8, 8, 128)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_110 (Dropout)            (None, 8, 8, 128)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_D2b (Conv2D)                (None, 8, 8, 128)     147584                                       \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_286 (BatchNo (None, 8, 8, 128)     512                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_252 (Activation)      (None, 8, 8, 128)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_94 (MaxPooling2D)  (None, 4, 4, 128)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_D3a (Conv2D)                (None, 4, 4, 256)     295168                                       \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_289 (BatchNo (None, 4, 4, 256)     1024                                         \n",
      "____________________________________________________________________________________________________\n",
      "activation_255 (Activation)      (None, 4, 4, 256)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_112 (Dropout)            (None, 4, 4, 256)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_D3b (Conv2D)                (None, 4, 4, 256)     590080                                       \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_290 (BatchNo (None, 4, 4, 256)     1024                                         \n",
      "____________________________________________________________________________________________________\n",
      "activation_256 (Activation)      (None, 4, 4, 256)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_96 (MaxPooling2D)  (None, 2, 2, 256)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_32 (Flatten)             (None, 1024)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_189 (Dense)                (None, 10)            10250                                        \n",
      "====================================================================================================\n",
      "Total params: 4,824,229.0\n",
      "Trainable params: 4,818,585.0\n",
      "Non-trainable params: 5,644.0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import autoencoder\n",
    "from imp import reload\n",
    "reload(autoencoder)\n",
    "\n",
    "# aeclass = autoencoder.VAE_MNIST_0(batch_size=100, n_stacks=0, compile_decoder=False)\n",
    "aeclass = DiscVAE(input_shape=x_train[0].shape, n_stacks=4, n_classes=y_train_oh.shape[1])\n",
    "print(aeclass.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9997cf906b94a28a329fbd99fb4a268"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3066c0efd304397b79f76ea27adca12"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aeclass.model.fit(x_train, y_train, batch_size=100, nb_epoch=50, verbose=0, callbacks=[TQDMNotebookCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pp = aeclass.model.predict(x_test, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.903216\n",
      "0.0 0.124758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f60537b0d30>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6053801ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 111\n",
    "if x_test.shape[-1] == 1:\n",
    "    s = pp.shape\n",
    "    pp = pp.reshape((s[0], s[1], s[2]))\n",
    "    x_test = x_test.reshape((len(x_test), s[1], s[2]))\n",
    "    \n",
    "print(np.amax(x_test[n]), np.amax(pp[n]))\n",
    "print(np.amin(x_test[n]), np.amin(pp[n]))\n",
    "plt.imshow(x_test[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6053711ac8>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niERR8AuRKAp+IRJFwS9Eoij4hUiU/wMXGWIRnZ075gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f60538a65c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(pp[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_13 (InputLayer)            (100, 28, 28, 1)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_39 (Convolution2D) (100, 28, 28, 1)      5           input_13[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_40 (Convolution2D) (100, 14, 14, 64)     320         convolution2d_39[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_41 (Convolution2D) (100, 14, 14, 64)     36928       convolution2d_40[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_42 (Convolution2D) (100, 14, 14, 64)     36928       convolution2d_41[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)             (100, 12544)          0           convolution2d_42[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_72 (Dense)                 (100, 128)            1605760     flatten_13[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_73 (Dense)                 (100, 2)              258         dense_72[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_74 (Dense)                 (100, 2)              258         dense_72[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)                (100, 2)              0           dense_73[0][0]                   \n",
      "                                                                   dense_74[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_75 (Dense)                 (100, 128)            384         lambda_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_76 (Dense)                 (100, 12544)          1618176     dense_75[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)             (100, 14, 14, 64)     0           dense_76[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "deconvolution2d_37 (Deconvolutio (100, 14, 14, 64)     36928       reshape_13[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "deconvolution2d_38 (Deconvolutio (100, 14, 14, 64)     36928       deconvolution2d_37[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "deconvolution2d_39 (Deconvolutio (100, 29, 29, 64)     16448       deconvolution2d_38[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_43 (Convolution2D) (100, 28, 28, 1)      257         deconvolution2d_39[0][0]         \n",
      "====================================================================================================\n",
      "Total params: 3,389,578\n",
      "Trainable params: 3,389,578\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''This script demonstrates how to build a variational autoencoder\n",
    "with Keras and deconvolution layers.\n",
    "\n",
    "Reference: \"Auto-Encoding Variational Bayes\" https://arxiv.org/abs/1312.6114\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape\n",
    "from keras.layers import Convolution2D, Deconvolution2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols, img_chns = 28, 28, 1\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 64\n",
    "# convolution kernel size\n",
    "nb_conv = 3\n",
    "\n",
    "batch_size = 100\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    original_img_size = (img_chns, img_rows, img_cols)\n",
    "else:\n",
    "    original_img_size = (img_rows, img_cols, img_chns)\n",
    "latent_dim = 2\n",
    "intermediate_dim = 128\n",
    "epsilon_std = 0.01\n",
    "nb_epoch = 2\n",
    "\n",
    "x = Input(batch_shape=(batch_size,) + original_img_size)\n",
    "conv_1 = Convolution2D(img_chns, 2, 2, border_mode='same', activation='relu')(x)\n",
    "conv_2 = Convolution2D(nb_filters, 2, 2,\n",
    "                       border_mode='same', activation='relu',\n",
    "                       subsample=(2, 2))(conv_1)\n",
    "conv_3 = Convolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                       border_mode='same', activation='relu',\n",
    "                       subsample=(1, 1))(conv_2)\n",
    "conv_4 = Convolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                       border_mode='same', activation='relu',\n",
    "                       subsample=(1, 1))(conv_3)\n",
    "flat = Flatten()(conv_4)\n",
    "hidden = Dense(intermediate_dim, activation='relu')(flat)\n",
    "\n",
    "z_mean = Dense(latent_dim)(hidden)\n",
    "z_log_var = Dense(latent_dim)(hidden)\n",
    "\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim),\n",
    "                              mean=0., std=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "# so you could write `Lambda(sampling)([z_mean, z_log_var])`\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# we instantiate these layers separately so as to reuse them later\n",
    "decoder_hid = Dense(intermediate_dim, activation='relu')\n",
    "decoder_upsample = Dense(nb_filters * 14 * 14, activation='relu')\n",
    "\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    output_shape = (batch_size, nb_filters, 14, 14)\n",
    "else:\n",
    "    output_shape = (batch_size, 14, 14, nb_filters)\n",
    "\n",
    "decoder_reshape = Reshape(output_shape[1:])\n",
    "decoder_deconv_1 = Deconvolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                                   output_shape,\n",
    "                                   border_mode='same',\n",
    "                                   subsample=(1, 1),\n",
    "                                   activation='relu')\n",
    "decoder_deconv_2 = Deconvolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                                   output_shape,\n",
    "                                   border_mode='same',\n",
    "                                   subsample=(1, 1),\n",
    "                                   activation='relu')\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    output_shape = (batch_size, nb_filters, 29, 29)\n",
    "else:\n",
    "    output_shape = (batch_size, 29, 29, nb_filters)\n",
    "decoder_deconv_3_upsamp = Deconvolution2D(nb_filters, 2, 2,\n",
    "                                          output_shape,\n",
    "                                          border_mode='valid',\n",
    "                                          subsample=(2, 2),\n",
    "                                          activation='relu')\n",
    "decoder_mean_squash = Convolution2D(img_chns, 2, 2,\n",
    "                                    border_mode='valid',\n",
    "                                    activation='sigmoid')\n",
    "\n",
    "hid_decoded = decoder_hid(z)\n",
    "up_decoded = decoder_upsample(hid_decoded)\n",
    "reshape_decoded = decoder_reshape(up_decoded)\n",
    "deconv_1_decoded = decoder_deconv_1(reshape_decoded)\n",
    "deconv_2_decoded = decoder_deconv_2(deconv_1_decoded)\n",
    "x_decoded_relu = decoder_deconv_3_upsamp(deconv_2_decoded)\n",
    "x_decoded_mean_squash = decoder_mean_squash(x_decoded_relu)\n",
    "\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    # NOTE: binary_crossentropy expects a batch_size by dim\n",
    "    # for x and x_decoded_mean, so we MUST flatten these!\n",
    "    x = K.flatten(x)\n",
    "    x_decoded_mean = K.flatten(x_decoded_mean)\n",
    "    xent_loss = img_rows * img_cols * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "vae = Model(x, x_decoded_mean_squash)\n",
    "vae.compile(optimizer='rmsprop', loss=vae_loss)\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "x_train.shape: (60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# train the VAE on MNIST digits\n",
    "(x_train, _), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train[:, :28, :28]\n",
    "x_test = x_test[:, :28, :28]\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape((x_train.shape[0],) + original_img_size)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape((x_test.shape[0],) + original_img_size)\n",
    "\n",
    "print('x_train.shape:', x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb9ba9a2bf4462faca46d23de7ba287"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5413deeb4c24729af1c2af7b895be7c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "          \r",
      "269/|/[loss: nan]  45%|| 269/600 [00:19<00:06, 51.58it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1359be6df24d3aa78bc0b9377bd8e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-fbed013a1df9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m        verbose=0, callbacks=[TQDMNotebookCallback()])\n\u001b[0m",
      "\u001b[0;32m/home/mike/ve/ml/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/ve/ml/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m    841\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/ve/ml/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 1603\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   1604\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/ve/ml/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/ve/ml/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/ve/ml/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/mike/ve/ml/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/ve/ml/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vae.fit(x1, x1,\n",
    "        shuffle=True,\n",
    "        nb_epoch=nb_epoch,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, x_test), \n",
    "       verbose=0, callbacks=[TQDMNotebookCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# build a model to project inputs on the latent space\n",
    "encoder = Model(x, z_mean)\n",
    "\n",
    "# display a 2D plot of the digit classes in the latent space\n",
    "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# build a digit generator that can sample from the learned distribution\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_hid_decoded = decoder_hid(decoder_input)\n",
    "_up_decoded = decoder_upsample(_hid_decoded)\n",
    "_reshape_decoded = decoder_reshape(_up_decoded)\n",
    "_deconv_1_decoded = decoder_deconv_1(_reshape_decoded)\n",
    "_deconv_2_decoded = decoder_deconv_2(_deconv_1_decoded)\n",
    "_x_decoded_relu = decoder_deconv_3_upsamp(_deconv_2_decoded)\n",
    "_x_decoded_mean_squash = decoder_mean_squash(_x_decoded_relu)\n",
    "generator = Model(decoder_input, _x_decoded_mean_squash)\n",
    "\n",
    "# display a 2D manifold of the digits\n",
    "n = 15  # figure with 15x15 digits\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n, x_train.shape[3]))\n",
    "# we will sample n points within [-15, 15] standard deviations\n",
    "grid_x = np.linspace(-15, 15, n)\n",
    "grid_y = np.linspace(-15, 15, n)\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        z_sample = np.tile(z_sample, batch_size).reshape(batch_size, 2)\n",
    "        x_decoded = generator.predict(z_sample, batch_size=batch_size)\n",
    "        digit = x_decoded[0].reshape((digit_size, digit_size, -1))\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
