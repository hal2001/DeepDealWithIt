{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "# os.environ['KERAS_BACKEND'] = 'theano'\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "from tensorflow import test\n",
    "test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10, mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "(x_train, _), (x_test, y_test) = cifar10.load_data()\n",
    "if x_train.ndim == 3:\n",
    "    s = x_train.shape\n",
    "    x_train = x_train.reshape((len(x_train), s[1], s[2], 1))\n",
    "    x_test = x_test.reshape((len(x_test), s[1], s[2], 1))\n",
    "original_img_size = x_train[0].shape\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape((x_train.shape[0],) + original_img_size)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape((x_test.shape[0],) + original_img_size)\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal, ndimage, misc\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape, BatchNormalization, Dropout, GaussianNoise\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Conv2D, Deconv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.layers import Conv3D, UpSampling3D, MaxPooling3D\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras import backend as K_backend\n",
    "from keras import objectives\n",
    "\n",
    "import models_basic\n",
    "\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "from autoencoder import VAE_MNIST_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Autoencoder(models_basic.SaveableModel):\n",
    "    \"\"\"\n",
    "    Base class for all-purpose autoencoder. VAE, CNN-AE, etc will be built off of this.\n",
    "\n",
    "    Input -> Encoder -> Z Latent Vector -> Decoder -> Output\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_shape=(28, 28, 1),\n",
    "                 latent_dim=2,  # Size of the encoded vector\n",
    "                 batch_size=100, # size of minibatch\n",
    "                 compile_decoder=False # create the decoder. Not necessary for every use case\n",
    "                 ):\n",
    "        self.model = None\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.batch_size = batch_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.compile_decoder = compile_decoder\n",
    "        assert K_backend.image_dim_ordering() == 'tf', 'Cannot support Theano ordering! Use TF ordering! #tensorflowmasterrace'\n",
    "\n",
    "        # input image dimensions\n",
    "        self.input_shape = input_shape\n",
    "        # self.data_shape = input_shape[1:] # Shape of a single sample\n",
    "        if len(input_shape) == 4:\n",
    "            self.img_rows, self.img_cols, self.img_stacks, self.img_chns = input_shape\n",
    "        elif len(input_shape) == 3:\n",
    "            self.img_rows, self.img_cols, self.img_chns = input_shape\n",
    "        elif len(input_shape) == 2:\n",
    "            self.img_rows, self.img_cols = input_shape\n",
    "            self.img_chns = 1\n",
    "        elif len(input_shape) == 1:\n",
    "            self.img_rows = input_shape[0]  # todo: test this\n",
    "        else:\n",
    "            raise ValueError(\"Invalid input shape: {}\".format(input_shape))\n",
    "\n",
    "    def rollup_decoder(self, z, z_input, layers_list):\n",
    "        \"\"\"\n",
    "        Takes a list of Keras layers and returns the decoder back-half and the standalone decoder model\n",
    "        :param z: Layer corresponding to the latent space vector\n",
    "        :param z_input: Layer corresponding to the decoder input\n",
    "        :param layers_list: List of layers to roll up\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        ae = AE_Dec()\n",
    "        dc = AE_Dec()\n",
    "        last_ae = z\n",
    "        last_dc = z_input\n",
    "        for i, layer in enumerate(layers_list):\n",
    "            #             if i ==0:\n",
    "            last_ae = layer(last_ae)\n",
    "            if self.compile_decoder:\n",
    "                last_dc = layer(last_dc)\n",
    "        return last_ae, last_dc\n",
    "\n",
    "\n",
    "class AE_Dec(object):\n",
    "    \"\"\"\n",
    "    Dummy object for reasons I can't remember. This may be deprecated.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class VAE(Autoencoder):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_shape=(28, 28, 1),\n",
    "                 latent_dim=2,  # Size of the encoded vector\n",
    "                 batch_size=100,  # size of minibatch\n",
    "                 epsilon_std=1.0, # This is the stddev for our normal-dist sampling of the latent vector\n",
    "                 compile_decoder=False\n",
    "                 ):\n",
    "        super().__init__(input_shape=input_shape, latent_dim=latent_dim, batch_size=batch_size,\n",
    "                         compile_decoder=compile_decoder)\n",
    "        # Necessary to instantiate this as instance variables such that they can be passed to the loss function (internally), since loss functions are\n",
    "        # all of the form lossfn(y_true, y_pred)\n",
    "        self.epsilon_std = epsilon_std\n",
    "        self.z_mean = Dense(latent_dim)\n",
    "        self.z_log_var = Dense(latent_dim)\n",
    "\n",
    "\n",
    "\n",
    "    def sampling(self, args):\n",
    "        \"\"\"\n",
    "        This is what makes the variational technique happen.\n",
    "        :param args:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Forging our latent vector from the reparameterized mean and std requires some sampling trickery\n",
    "        # that admittedly I do not understand in the slightest at this point in time\n",
    "        z_mean, z_log_var = args\n",
    "        batch_size = self.batch_size\n",
    "        epsilon = K_backend.random_normal(shape=(batch_size, self.latent_dim),\n",
    "                                          mean=0., stddev=self.epsilon_std)\n",
    "        # We return z_mean + epsilon*sigma^2. Not sure why we use log var\n",
    "        # Basically, create a random variable vector from the distribution\n",
    "        # We are learning a distribution (mu, var) which represents the input\n",
    "        return z_mean + K_backend.exp(z_log_var) * epsilon\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean):\n",
    "        \"\"\"\n",
    "        Custom loss function for VAE. Uses Kullback-Leibler divergence.\n",
    "\n",
    "        Notes from fchollet: binary_crossentropy expects a shape (batch_size, dim) for x and x_decoded_mean,\n",
    "        so we MUST flatten these!\n",
    "        :param x:\n",
    "        :param x_decoded_mean:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        x = K_backend.flatten(x)\n",
    "        x_decoded_mean = K_backend.flatten(x_decoded_mean)\n",
    "        shape_coef = np.product(self.input_shape)\n",
    "        xent_loss = shape_coef * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "        kl_loss = - 0.5 * K_backend.mean(\n",
    "            1 + self.z_log_var - K_backend.square(self.z_mean) - K_backend.exp(self.z_log_var), axis=-1)\n",
    "        return xent_loss + kl_loss\n",
    "\n",
    "\n",
    "class DiscVAE(VAE):\n",
    "    \"\"\" Covolutional Discriminative VAE\n",
    "     Discriminative Regularization for Generative Models\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_shape=(64, 64, 3),\n",
    "                 latent_dim=256,  # Size of the encoded vector\n",
    "                 n_classes=10,  # number of classes in dataset\n",
    "                 batch_size=100,  # size of minibatch\n",
    "                 n_stacks=3,  # Number of convolayers to stack, this boosts performance of the network dramatically\n",
    "                 intermediate_dim=256,  # Size of the dense layer after convs\n",
    "                 n_filters=64,  # Number of filters in the first layer\n",
    "                 px_conv=3,  # Default convolution window size\n",
    "                 dropout_p=0.1,  # Default dropout rate\n",
    "                 epsilon_std=1.0,  # This is the stddev for our normal-dist sampling of the latent vector\n",
    "                 compile_decoder=True,\n",
    "                 ):\n",
    "\n",
    "        # This is my original crossfire network, and it works. As such, it has apprentice marks all over\n",
    "        # Reconstructing as-is before tinkering\n",
    "        # Based heavily on https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder_deconv.py\n",
    "        # and https://groups.google.com/forum/#!msg/keras-users/iBp3Ngxll3k/_GbY4nqNCQAJ\n",
    "\n",
    "        super().__init__(input_shape=input_shape, latent_dim=latent_dim, batch_size=batch_size, epsilon_std=epsilon_std,\n",
    "                         compile_decoder=compile_decoder)\n",
    "        self.new_model(input_shape=input_shape, latent_dim=latent_dim, n_stacks=n_stacks)\n",
    "\n",
    "\n",
    "    def new_model(self,\n",
    "                  input_shape=(64,64,3),\n",
    "                  latent_dim=256,\n",
    "                  kern=3,\n",
    "                  n_filtersX=32,\n",
    "                  n_stacks=4,\n",
    "                  dropout_p=0.2,\n",
    "                  intermediate_dim=256):\n",
    "        # Convolutional frontend filters as per typical convonets\n",
    "        print(self.input_shape)\n",
    "\n",
    "        core_width = input_shape[0] // 2**n_stacks # this will probably break horribly for shapes not power of 2\n",
    "        n_filtersZ = n_filtersX * 2**(n_stacks-1)\n",
    "\n",
    "        x_in = Input(self.input_shape, name='main_input')\n",
    "        stack = x_in # bit of a hack so we can use a for loop here\n",
    "        \n",
    "        enc_list = [BatchNormalization()]\n",
    "\n",
    "        # -> CONV/FC -> BatchNorm -> ReLu(or other activation) -> Dropout -> CONV/FC ->\n",
    "        for i in range(n_stacks):\n",
    "            enc_list.append(Conv2D(n_filtersX * 2 ** i, (kern, kern), padding='same', activation='relu', name='conv_{}a'.format(i)))\n",
    "#             stack = BatchNormalization()(stack)\n",
    "#             stack = Activation('relu')(stack)\n",
    "            stack = Dropout(dropout_p)(stack)\n",
    "            stack = Conv2D(n_filtersX * 2 ** i, (kern, kern), padding='same', activation='relu', name='conv_{}b'.format(i))(stack)\n",
    "#             stack = BatchNormalization()(stack)\n",
    "#             stack = Activation('relu')(stack)\n",
    "            stack = Dropout(dropout_p)(stack)\n",
    "            stack = MaxPooling2D(pool_size=(2,2))(stack)\n",
    "\n",
    "\n",
    "\n",
    "        # Densely connected layer after the filters\n",
    "        flat = Flatten()(stack)\n",
    "        hidden_1 = Dense(intermediate_dim, activation='relu', name='intermezzo')(flat)\n",
    "\n",
    "        # This is the Variational Autoencoder reparameterization trick\n",
    "        z_mean = Dense(latent_dim)(hidden_1)\n",
    "        z_log_var = Dense(latent_dim)(hidden_1)\n",
    "\n",
    "        # Make these instance vars so X-Ent can use them. Probably a better way out there\n",
    "        self.z_mean = z_mean\n",
    "        self.z_log_var = z_log_var\n",
    "\n",
    "#         # Part 2 of the reparam trick is sample from the mean-vec and std-vec (log_var). To do this, we utilize a\n",
    "#         # custom layer via Lambda class to combine the mean and log_var outputs and a custom sampling function\n",
    "#         # 'z' is our latent vector\n",
    "        z = Lambda(self.sampling, output_shape=(latent_dim,), name='latent_z')([z_mean, z_log_var])\n",
    "#         print(z.shape)\n",
    "\n",
    "#         # This marks the end of the encoding portion of the VAE\n",
    "\n",
    "#         # On to Decoder. we instantiate these layers separately so as to reuse them later\n",
    "#         # e.g. for feeding in latent-space vectors, or (presumably) inspecting output\n",
    "\n",
    "        output_shape = (None, core_width, core_width, n_filtersZ)\n",
    "\n",
    "        decoder_hidden = Dense(intermediate_dim, activation='relu')\n",
    "        decoder_upsample = Dense(n_filtersZ * core_width**2, activation='relu')\n",
    "\n",
    "\n",
    "        decoder_reshape = Reshape(output_shape[1:])\n",
    "\n",
    "        dec_list = [decoder_hidden, decoder_upsample, decoder_reshape]\n",
    "\n",
    "        for i in range(n_stacks-1):\n",
    "            dec_list.append(UpSampling2D((2,2)))\n",
    "            dec_list.append(Deconv2D(n_filtersX * 2 ** (n_stacks-i-1), (kern, kern), padding='same', activation='relu', name='deconv_{}a'.format(i)))\n",
    "            dec_list.append(BatchNormalization())\n",
    "            dec_list.append(Activation('relu'))\n",
    "            dec_list.append(Dropout(dropout_p))\n",
    "            dec_list.append(Deconv2D(n_filtersX * 2 ** (n_stacks-i-1), (kern, kern), padding='same', activation='relu', name='deconv_{}b'.format(i)))\n",
    "            dec_list.append(BatchNormalization())\n",
    "            dec_list.append(Activation('relu'))\n",
    "            dec_list.append(Dropout(dropout_p))\n",
    "            \n",
    "        dec_list.append(UpSampling2D((2,2)))\n",
    "        dec_list.append(Conv2D(self.input_shape[2], (kern, kern), padding='same', activation='sigmoid', name='deconv_fin')) # trick to drop down to N channels from filters\n",
    "\n",
    "#         # # FC uses Deconv, but another example uses UpSample layers. See Keras Api: Deconvolution2D\n",
    "#         # decoder_deconv_1 = Deconv2D(n_filtersX, kern, kern, output_shape,\n",
    "#         #                             border_mode='same', activation='relu')\n",
    "#         # decoder_deconv_2 = Deconv2D(n_filtersX, kern, kern, output_shape,\n",
    "#         #                             border_mode='same', activation='relu')\n",
    "#         #\n",
    "#         # # Some more reshaping, presumably I need to modify this in order to use different shapes\n",
    "#         # output_shape = (None, 29, 29, n_filtersX)\n",
    "#         #\n",
    "#         # # more FC voodoo\n",
    "#         # decoder_deconv_3_upsamp = Deconv2D(n_filtersX, 2, 2, output_shape, border_mode='valid', subsample=(2, 2),\n",
    "#         #                                    activation='relu')\n",
    "#         # decoder_mean_squash = Conv2D(self.img_chns, 2, 2, border_mode='same', activation='sigmoid', name='main_output')\n",
    "#         #\n",
    "#         #\n",
    "#         # x_decoded_mean_squash = decoder_mean_squash\n",
    "\n",
    "#         # layers_list = [decoder_hidden, decoder_upsample, decoder_reshape, decoder_deconv_1, decoder_deconv_2,\n",
    "#         #                decoder_deconv_3_upsamp, decoder_mean_squash]\n",
    "\n",
    "        decoder_input = Input(shape=(latent_dim,))\n",
    "\n",
    "        # todo: better naming convention\n",
    "        ae, dc = self.rollup_decoder(z, decoder_input, dec_list)\n",
    "\n",
    "#         # if self.compile_decoder:\n",
    "#         #     # FC: build a digit generator that can sample from the learned distribution\n",
    "#         #     # todo: (un)roll this\n",
    "#         #     _hid_decoded = decoder_hidden(decoder_input)\n",
    "#         #     _up_decoded = decoder_upsample(_hid_decoded)\n",
    "#         #     _reshape_decoded = decoder_reshape(_up_decoded)\n",
    "#         #     _deconv_1_decoded = decoder_deconv_1(_reshape_decoded)\n",
    "#         #     _deconv_2_decoded = decoder_deconv_2(_deconv_1_decoded)\n",
    "#         #     _x_decoded_relu = decoder_deconv_3_upsamp(_deconv_2_decoded)\n",
    "#         #     _x_decoded_mean_squash = decoder_mean_squash(_x_decoded_relu)\n",
    "\n",
    "#         # Now we create the actual models. We also compile them automatically, this could be isolated later\n",
    "#         # Primary model - VAE\n",
    "        self.model = Model(x_in, ae)\n",
    "        self.model.compile(optimizer='rmsprop', loss=self.vae_loss)\n",
    "\n",
    "\n",
    "        # build a model to project inputs on the latent space\n",
    "        self.encoder = Model(x_in, self.z_mean)\n",
    "#         self.foo = Model(x_in, decoder_reshape)\n",
    "        if 0:# self.compile_decoder:\n",
    "            # reconstruct the digit pictures from latent space\n",
    "            self.decoder = Model(decoder_input, dc)\n",
    "\n",
    "    def fit_crossmodel(self, x_dict, y_dict, batch_size=None, nb_epoch=10, verbose=1, callbacks=[],\n",
    "                       validation_split=0.,\n",
    "                       validation_data=None, shuffle=True, class_weight=None, sample_weight=None):\n",
    "        pass\n",
    "        #     callbacks_history = self.crossmodel.fit(x_dict, y_dict, batch_size, nb_epoch, verbose, callbacks,\n",
    "        #                                             validation_split,\n",
    "        #                                             validation_data, shuffle, class_weight, sample_weight)\n",
    "        #     return callbacks_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 32, 32, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 32, 32, 3)     12                                           \n",
      "____________________________________________________________________________________________________\n",
      "conv_0a (Conv2D)                 (None, 32, 32, 32)    896                                          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 32, 32, 32)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_0b (Conv2D)                 (None, 32, 32, 32)    9248                                         \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 32, 32, 32)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)   (None, 16, 16, 32)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_1a (Conv2D)                 (None, 16, 16, 64)    18496                                        \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv_1b (Conv2D)                 (None, 16, 16, 64)    36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)   (None, 8, 8, 64)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 4096)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "intermezzo (Dense)               (None, 256)           1048832                                      \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 256)           65792                                        \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 256)           65792                                        \n",
      "____________________________________________________________________________________________________\n",
      "latent_z (Lambda)                (None, 256)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 256)           65792                                        \n",
      "____________________________________________________________________________________________________\n",
      "dense_12 (Dense)                 (None, 4096)          1052672                                      \n",
      "____________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)              (None, 8, 8, 64)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)   (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "deconv_0a (Conv2DTranspose)      (None, 16, 16, 64)    36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 16, 16, 64)    256                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)              (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "deconv_0b (Conv2DTranspose)      (None, 16, 16, 64)    36928                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 16, 16, 64)    256                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)             (None, 16, 16, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)   (None, 32, 32, 64)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "deconv_fin (Conv2D)              (None, 32, 32, 3)     1731                                         \n",
      "====================================================================================================\n",
      "Total params: 2,440,559.0\n",
      "Trainable params: 2,440,297.0\n",
      "Non-trainable params: 262.0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import autoencoder\n",
    "from imp import reload\n",
    "reload(autoencoder)\n",
    "\n",
    "# aeclass = autoencoder.VAE_MNIST_0(batch_size=100, n_stacks=0, compile_decoder=False)\n",
    "aeclass = DiscVAE(input_shape=x_train[0].shape, n_stacks=2)\n",
    "print(aeclass.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bcaad28ab1848839c8a9c0e8869d611"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b4628b6c5d436c93ba54e94e96e037"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c3aeb6c7cc4f7890ab2dcb0e8036ff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d76fc77b43c040279fb39f07c50220c3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c9d4bac6fd94c848a2a9b188eed4730"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eea8c8b997841c698dc1cc16924f58f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49678efa2c354bafa7f2a502ab8aa8d8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61b9a9d3c1a408580a9dbc525a010c4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c99e11413524098960045c4ff61e8f1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3f2d876a5747e4b9892cf02f9fba33"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1610a1bb84b644c8a1ae3c96ebfe980a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23608b36d664e999c32ca28776791b2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13151567e7c4728a5fb5e6487944481"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430a5a49394a40f3ae62da0a32a50c63"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8eadcd5a62e470bab47b123ee107000"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae9647d172a4c64bee263d27edf116a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c908e0e397ea4168ab276401a4de34f3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2182aa6c4314e35a59a0d5c40d2f412"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb172c312274a8ea72760ac744499a3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba932733a394538b2ae125ef671b6bb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7445c6978a994fb3a5941c121edbd3ae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f608c537438>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aeclass.model.fit(x_train, x_train, batch_size=100, nb_epoch=20, verbose=0, callbacks=[TQDMNotebookCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pp = aeclass.model.predict(x_test, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.827451 0.683652\n",
      "0.0 0.197774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f608c146278>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncn4hEkXOL0SiyPmFSBQ5vxCJ8v8AaqAOfezsxYcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f608c2c04e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 5\n",
    "if x_test.shape[-1] == 1:\n",
    "    s = pp.shape\n",
    "    pp = pp.reshape((s[0], s[1], s[2]))\n",
    "    x_test = x_test.reshape((len(x_test), s[1], s[2]))\n",
    "    \n",
    "print(np.amax(x_test[n]), np.amax(pp[n]))\n",
    "print(np.amin(x_test[n]), np.amin(pp[n]))\n",
    "plt.imshow(x_test[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f608c128a20>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f608c1f6a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(pp[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_13 (InputLayer)            (100, 28, 28, 1)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_39 (Convolution2D) (100, 28, 28, 1)      5           input_13[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_40 (Convolution2D) (100, 14, 14, 64)     320         convolution2d_39[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_41 (Convolution2D) (100, 14, 14, 64)     36928       convolution2d_40[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_42 (Convolution2D) (100, 14, 14, 64)     36928       convolution2d_41[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)             (100, 12544)          0           convolution2d_42[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_72 (Dense)                 (100, 128)            1605760     flatten_13[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_73 (Dense)                 (100, 2)              258         dense_72[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_74 (Dense)                 (100, 2)              258         dense_72[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)                (100, 2)              0           dense_73[0][0]                   \n",
      "                                                                   dense_74[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_75 (Dense)                 (100, 128)            384         lambda_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_76 (Dense)                 (100, 12544)          1618176     dense_75[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)             (100, 14, 14, 64)     0           dense_76[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "deconvolution2d_37 (Deconvolutio (100, 14, 14, 64)     36928       reshape_13[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "deconvolution2d_38 (Deconvolutio (100, 14, 14, 64)     36928       deconvolution2d_37[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "deconvolution2d_39 (Deconvolutio (100, 29, 29, 64)     16448       deconvolution2d_38[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_43 (Convolution2D) (100, 28, 28, 1)      257         deconvolution2d_39[0][0]         \n",
      "====================================================================================================\n",
      "Total params: 3,389,578\n",
      "Trainable params: 3,389,578\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''This script demonstrates how to build a variational autoencoder\n",
    "with Keras and deconvolution layers.\n",
    "\n",
    "Reference: \"Auto-Encoding Variational Bayes\" https://arxiv.org/abs/1312.6114\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape\n",
    "from keras.layers import Convolution2D, Deconvolution2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols, img_chns = 28, 28, 1\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 64\n",
    "# convolution kernel size\n",
    "nb_conv = 3\n",
    "\n",
    "batch_size = 100\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    original_img_size = (img_chns, img_rows, img_cols)\n",
    "else:\n",
    "    original_img_size = (img_rows, img_cols, img_chns)\n",
    "latent_dim = 2\n",
    "intermediate_dim = 128\n",
    "epsilon_std = 0.01\n",
    "nb_epoch = 2\n",
    "\n",
    "x = Input(batch_shape=(batch_size,) + original_img_size)\n",
    "conv_1 = Convolution2D(img_chns, 2, 2, border_mode='same', activation='relu')(x)\n",
    "conv_2 = Convolution2D(nb_filters, 2, 2,\n",
    "                       border_mode='same', activation='relu',\n",
    "                       subsample=(2, 2))(conv_1)\n",
    "conv_3 = Convolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                       border_mode='same', activation='relu',\n",
    "                       subsample=(1, 1))(conv_2)\n",
    "conv_4 = Convolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                       border_mode='same', activation='relu',\n",
    "                       subsample=(1, 1))(conv_3)\n",
    "flat = Flatten()(conv_4)\n",
    "hidden = Dense(intermediate_dim, activation='relu')(flat)\n",
    "\n",
    "z_mean = Dense(latent_dim)(hidden)\n",
    "z_log_var = Dense(latent_dim)(hidden)\n",
    "\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim),\n",
    "                              mean=0., std=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "# so you could write `Lambda(sampling)([z_mean, z_log_var])`\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# we instantiate these layers separately so as to reuse them later\n",
    "decoder_hid = Dense(intermediate_dim, activation='relu')\n",
    "decoder_upsample = Dense(nb_filters * 14 * 14, activation='relu')\n",
    "\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    output_shape = (batch_size, nb_filters, 14, 14)\n",
    "else:\n",
    "    output_shape = (batch_size, 14, 14, nb_filters)\n",
    "\n",
    "decoder_reshape = Reshape(output_shape[1:])\n",
    "decoder_deconv_1 = Deconvolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                                   output_shape,\n",
    "                                   border_mode='same',\n",
    "                                   subsample=(1, 1),\n",
    "                                   activation='relu')\n",
    "decoder_deconv_2 = Deconvolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                                   output_shape,\n",
    "                                   border_mode='same',\n",
    "                                   subsample=(1, 1),\n",
    "                                   activation='relu')\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    output_shape = (batch_size, nb_filters, 29, 29)\n",
    "else:\n",
    "    output_shape = (batch_size, 29, 29, nb_filters)\n",
    "decoder_deconv_3_upsamp = Deconvolution2D(nb_filters, 2, 2,\n",
    "                                          output_shape,\n",
    "                                          border_mode='valid',\n",
    "                                          subsample=(2, 2),\n",
    "                                          activation='relu')\n",
    "decoder_mean_squash = Convolution2D(img_chns, 2, 2,\n",
    "                                    border_mode='valid',\n",
    "                                    activation='sigmoid')\n",
    "\n",
    "hid_decoded = decoder_hid(z)\n",
    "up_decoded = decoder_upsample(hid_decoded)\n",
    "reshape_decoded = decoder_reshape(up_decoded)\n",
    "deconv_1_decoded = decoder_deconv_1(reshape_decoded)\n",
    "deconv_2_decoded = decoder_deconv_2(deconv_1_decoded)\n",
    "x_decoded_relu = decoder_deconv_3_upsamp(deconv_2_decoded)\n",
    "x_decoded_mean_squash = decoder_mean_squash(x_decoded_relu)\n",
    "\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    # NOTE: binary_crossentropy expects a batch_size by dim\n",
    "    # for x and x_decoded_mean, so we MUST flatten these!\n",
    "    x = K.flatten(x)\n",
    "    x_decoded_mean = K.flatten(x_decoded_mean)\n",
    "    xent_loss = img_rows * img_cols * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "vae = Model(x, x_decoded_mean_squash)\n",
    "vae.compile(optimizer='rmsprop', loss=vae_loss)\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "x_train.shape: (60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# train the VAE on MNIST digits\n",
    "(x_train, _), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train[:, :28, :28]\n",
    "x_test = x_test[:, :28, :28]\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape((x_train.shape[0],) + original_img_size)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape((x_test.shape[0],) + original_img_size)\n",
    "\n",
    "print('x_train.shape:', x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb9ba9a2bf4462faca46d23de7ba287"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5413deeb4c24729af1c2af7b895be7c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "          \r",
      "269/|/[loss: nan]  45%|| 269/600 [00:19<00:06, 51.58it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1359be6df24d3aa78bc0b9377bd8e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-fbed013a1df9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m        verbose=0, callbacks=[TQDMNotebookCallback()])\n\u001b[0m",
      "\u001b[0;32m/home/mike/ve/ml/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/ve/ml/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m    841\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/ve/ml/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 1603\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   1604\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/ve/ml/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/ve/ml/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/ve/ml/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/mike/ve/ml/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mike/ve/ml/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vae.fit(x1, x1,\n",
    "        shuffle=True,\n",
    "        nb_epoch=nb_epoch,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, x_test), \n",
    "       verbose=0, callbacks=[TQDMNotebookCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# build a model to project inputs on the latent space\n",
    "encoder = Model(x, z_mean)\n",
    "\n",
    "# display a 2D plot of the digit classes in the latent space\n",
    "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# build a digit generator that can sample from the learned distribution\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_hid_decoded = decoder_hid(decoder_input)\n",
    "_up_decoded = decoder_upsample(_hid_decoded)\n",
    "_reshape_decoded = decoder_reshape(_up_decoded)\n",
    "_deconv_1_decoded = decoder_deconv_1(_reshape_decoded)\n",
    "_deconv_2_decoded = decoder_deconv_2(_deconv_1_decoded)\n",
    "_x_decoded_relu = decoder_deconv_3_upsamp(_deconv_2_decoded)\n",
    "_x_decoded_mean_squash = decoder_mean_squash(_x_decoded_relu)\n",
    "generator = Model(decoder_input, _x_decoded_mean_squash)\n",
    "\n",
    "# display a 2D manifold of the digits\n",
    "n = 15  # figure with 15x15 digits\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n, x_train.shape[3]))\n",
    "# we will sample n points within [-15, 15] standard deviations\n",
    "grid_x = np.linspace(-15, 15, n)\n",
    "grid_y = np.linspace(-15, 15, n)\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        z_sample = np.tile(z_sample, batch_size).reshape(batch_size, 2)\n",
    "        x_decoded = generator.predict(z_sample, batch_size=batch_size)\n",
    "        digit = x_decoded[0].reshape((digit_size, digit_size, -1))\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
